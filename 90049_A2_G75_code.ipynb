{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosfet-sudo/ML-Project-G75/blob/main/90049_A2_G75_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "note: the github must be public in order for us to view 'live updates' of any changes made to this notebook"
      ],
      "metadata": {
        "id": "m5JmuJbLWmzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount google drive\n",
        "run the code below to mount your google drive to this colab notebook for however long you need to be running the code for\n",
        "*Please accept Amelia's invitation to share the drive folder 'IML_G75_Colab_Notebooks*"
      ],
      "metadata": {
        "id": "z3I0q2j33C3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # To read in data from google drive (delete later?)\n",
        "drive.mount('/content/drive') # to read in data from google drive (delete later?)"
      ],
      "metadata": {
        "id": "Ru_ouPJqYHq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Data Processing\n",
        "(Done by Elijah)\n",
        "Each Dataset was checked for duplicate entries, and then specific numeric values were normalized. In addition to this there was some minor text processing of the NASA dataset where the text files were tranferred into csv format. Furthermore adjustments were made to the metrodataset where all data points of each minute were averaged in order to make the dataset smaller. In particular for values that were binary (i.e) 1 or 0 if the average was strictly greater than 0.5 then it was set to 1 otherwise it was set to 0. This averaging should reduce noise and reduce the affect of anomalous outliers."
      ],
      "metadata": {
        "id": "-glSLfYeXa2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metro-PT3"
      ],
      "metadata": {
        "id": "fILNNgYJYIM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The processed dataset has been uploaded to the drive already, might not be advisable to run this code (raw data file is too large)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "## averaging function for new dataset that preserves when errors occur, n must be even n=6 for averaging every minute\n",
        "## takes the data  frame the number of samples to bunch together and average and the indexes that are we are applying the averaging off.\n",
        "## the inputs are the data frame, an integer n, and 2 arrays of numeric indexes it returns the new (smaller) averaged dataframe\n",
        "def timeAvg(df, n, tempindexes, elec_temp_indexes):\n",
        "    m = n/2\n",
        "    m = int(m)\n",
        "\n",
        "    df_empty = df.copy(deep=True).iloc[0:0]\n",
        "    avg_atpoints = []\n",
        "    max_atpoints = []\n",
        "    for i in tempindexes:\n",
        "        print(i)\n",
        "        avg_atpoints = []\n",
        "        temp_series = df.iloc[:,i]\n",
        "        numrows = int(temp_series.shape[0])\n",
        "        for j in range(m, int(numrows-m),n):\n",
        "            avg_atpoints.append(temp_series.iloc[j-m:j+m].mean()) #here we take the average\n",
        "        df_empty.iloc[:,i] = avg_atpoints\n",
        "    for i in elec_temp_indexes:\n",
        "        print(i)\n",
        "        max_atpoints = []\n",
        "        temp_series = df.iloc[:,i]\n",
        "        numrows = int(temp_series.shape[0])\n",
        "        for j in range(m, int(numrows-m),n):\n",
        "            avg = temp_series.iloc[j-m:j+m].mean()  #we use the average to decide what to assign to this datapoint\n",
        "            if avg > 0.5:\n",
        "                max_atpoints.append(1)\n",
        "            else:\n",
        "                max_atpoints.append(0)\n",
        "        df_empty.iloc[:,i] = max_atpoints\n",
        "    df_empty.iloc[:,0] = list(range(1,df_empty.shape[0]+1))\n",
        "    return df_empty\n",
        "\n",
        "\n",
        "# df = pd.read_csv(\"C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/MetroPT3(AirCompressor).csv\")\n",
        "df = pd.read_csv('/content/drive/My Drive/IML_G75_Colab_Notebooks/')\n",
        "#Duplicate checking\n",
        "\n",
        "df = df.drop(columns = [\"timestamp\"])\n",
        "num_columns = df.shape[1]\n",
        "num_rows = df.shape[0]\n",
        "duplicateddf=df.duplicated()\n",
        "no_duplicate_df = df.loc[duplicateddf==False]\n",
        "\n",
        "\n",
        "no_duplicate_df.rename(columns={\"Unnamed: 0\": \"Time Elapased [seconds]\"}, inplace=True)\n",
        "print(no_duplicate_df.columns)\n",
        "\n",
        "# Normalisation\n",
        "# Find the indexes of columns which have values we want to normalize\n",
        "temp_indexes = [no_duplicate_df.columns.get_loc(\"Time Elapased [seconds]\")] ##SHOULD WE NORMALIsE THIS?\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"TP2\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"TP3\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"H1\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"DV_pressure\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Reservoirs\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Oil_temperature\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Motor_current\"))\n",
        "\n",
        "\n",
        "# BOTH RPM AND TOOLWEAR HAVE ISSUES WITH CERTAIN DTYPES BEING INT DUE TO WHOLE NUMBERS IN DATA\n",
        "no_duplicate_df[\"Time Elapased [seconds]\"] = no_duplicate_df[\"Time Elapased [seconds]\"].astype(float)\n",
        "\n",
        "#for each index normalize the values and for each row set the value to the normalized value\n",
        "for i in temp_indexes:\n",
        "    temp_series = no_duplicate_df.iloc[:,i]\n",
        "    series_max = temp_series.max()\n",
        "    series_min = temp_series.min()\n",
        "    series_range = temp_series.max() - temp_series.min()\n",
        "\n",
        "    for j in range(temp_series.shape[0]):\n",
        "\n",
        "        no_duplicate_df.iat[j,i] = (no_duplicate_df.iat[j,i]-series_min)/series_range\n",
        "\n",
        "\n",
        "\n",
        "## new temp indexes for finding columns of electrical signal data\n",
        "elec_temp_indexes = [no_duplicate_df.columns.get_loc(\"COMP\")]\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"DV_eletric\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Towers\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"MPG\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"LPS\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Pressure_switch\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Oil_level\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Caudal_impulses\"))\n",
        "\n",
        "\n",
        "\n",
        "no_dupe_normalized_df = no_duplicate_df\n",
        "no_dupe_normalized_df_avg = timeAvg(no_dupe_normalized_df,6,temp_indexes,elec_temp_indexes)\n",
        "\n",
        "no_dupe_normalized_df_avg.to_csv(\"METROPT3_AVGMINUTEPROCESSED.csv\")"
      ],
      "metadata": {
        "id": "-JPJp2XBYLL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NASA"
      ],
      "metadata": {
        "id": "iXTnSmEoYMNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Method for parsing text files into dataframe, takes a string input and returns a dataframe object\n",
        "def parseTxt2Dataframe(String):\n",
        "    with open(String, 'r', encoding='utf-8') as file:\n",
        "        lines = []\n",
        "        for line in file:\n",
        "            newline = line.strip()\n",
        "            newline = newline.split()\n",
        "            lines.append(newline)\n",
        "        new_df = pd.DataFrame(lines, columns=['Unit Number', 'Time in cycles',\n",
        "                                  'Operational Setting 1', 'Operational Setting 2','Operational Setting 3',\n",
        "                                  'T2','T24','T30','T50','P2','P15','P30','Nf','Nc','epr','Ps30','phi','NRf','NRc',\n",
        "                                  'BPR','farB','htBleed','Nf_dmd','PCNfr_dmd','W31','W32'])\n",
        "        num_cols = new_df.shape[1]\n",
        "        for i in range(num_cols):\n",
        "            new_df.iloc[:,i] = pd.to_numeric(new_df.iloc[:,i])\n",
        "\n",
        "\n",
        "    return new_df\n",
        "\n",
        "#normalizes the data, and checks for duplicates, takes dataframe object as input and returns a dataframe\n",
        "def normalize_and_checkdupe(df):\n",
        "    checkdf = df.drop(columns=['Unit Number','Time in cycles'])\n",
        "    duplicateddf = checkdf.duplicated()\n",
        "    no_dupe_df = df.loc[duplicateddf==False]\n",
        "    num_cols = no_dupe_df.shape[1]\n",
        "    num_rows = no_dupe_df.shape[0]\n",
        "    for i in range(2,num_cols):\n",
        "        max_val = no_dupe_df.iloc[:,i].max()\n",
        "        min_val = no_dupe_df.iloc[:,i].min()\n",
        "        val_range = max_val - min_val\n",
        "        for j in range(num_rows):\n",
        "            if val_range != 0:\n",
        "                no_dupe_df.iat[j, i] = (no_dupe_df.iat[j, i] - min_val) / val_range\n",
        "            else:\n",
        "                no_dupe_df.iat[j, i] = -1\n",
        "\n",
        "    return no_dupe_df\n",
        "\n",
        "\n",
        "# parsing each text file into a dataframe\n",
        "df_test1 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD001.txt')\n",
        "df_test2 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD002.txt')\n",
        "df_test3 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD003.txt')\n",
        "df_test4 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD004.txt')\n",
        "df_train1 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD001.txt')\n",
        "df_train2 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD002.txt')\n",
        "df_train3 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD003.txt')\n",
        "df_train4 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD004.txt')\n",
        "\n",
        "\n",
        "\n",
        "#combining the data into one dataset\n",
        "\n",
        "df_all = pd.concat([df_test1,df_test2,df_test3,df_test4,df_train1,df_train2,df_train3,df_train4],ignore_index=True)\n",
        "print(df_all.shape)\n",
        "print(df_test1.shape)\n",
        "\n",
        "## normalizing and checking for duplicates when combined\n",
        "df_all_processed = normalize_and_checkdupe(df_all)\n",
        "\n",
        "## normalizing and checking for duplicates when uncombined\n",
        "df_test1_processed = normalize_and_checkdupe(df_test1)\n",
        "#df_test2_processed = normalize_and_checkdupe(df_test2)\n",
        "#df_test3_processed = normalize_and_checkdupe(df_test3)\n",
        "#df_test4_processed = normalize_and_checkdupe(df_test4)\n",
        "#df_train1_processed = normalize_and_checkdupe(df_train1)\n",
        "#df_train2_processed = normalize_and_checkdupe(df_train2)\n",
        "#df_train3_processed = normalize_and_checkdupe(df_train3)\n",
        "#df_train4_processed = normalize_and_checkdupe(df_train4)\n",
        "\n",
        "\n",
        "\n",
        "## testing and debug\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(df_all_processed.shape)\n",
        "print(df_all_processed.head())\n",
        "print(df_test1_processed.head())\n",
        "print(\"newline\")\n",
        "\n",
        "\n",
        "print(\"newline\")\n",
        "print(df_all_processed.shape)\n",
        "\n",
        "pd.reset_option('display.max_columns')\n",
        "df_all_processed.to_csv('NASAprocessed.csv')"
      ],
      "metadata": {
        "id": "-XhCb94XYPf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MScFRuDoWV4a",
        "outputId": "1cd8f771-c840-46bc-9cc9-876e4cc4e152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PdM"
      ],
      "metadata": {
        "id": "NHgRsQ1jYP9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "df = pd.read_csv(\"C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/ai_2020.csv\")\n",
        "#Duplicate checking\n",
        "\n",
        "df = df.drop(columns = [\"UDI\"])\n",
        "num_columns = df.shape[1]\n",
        "num_rows = df.shape[0]\n",
        "duplicateddf=df.duplicated()\n",
        "no_duplicate_df = df.loc[duplicateddf==False]\n",
        "\n",
        "print(no_duplicate_df.columns)\n",
        "#Normalization\n",
        "\n",
        "temp_indexes = [no_duplicate_df.columns.get_loc(\"Air temperature [K]\")]\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Process temperature [K]\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Rotational speed [rpm]\")) #TURN INTO SECONDS?\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Torque [Nm]\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Tool wear [min]\"))     # TURN INTO SECONDS?\n",
        "# BOTH RPM AND TOOLWEAR HAVE ISSUES WITH CERTAIN DTYPES BEING INT DUE TO WHOLE NUMBERS IN DATA\n",
        "no_duplicate_df[\"Rotational speed [rpm]\"] = no_duplicate_df[\"Rotational speed [rpm]\"].astype(float)\n",
        "no_duplicate_df[\"Tool wear [min]\"] = no_duplicate_df[\"Tool wear [min]\"].astype(float)\n",
        "\n",
        "for i in temp_indexes:\n",
        "\n",
        "    temp_series = no_duplicate_df.iloc[:,i]\n",
        "    series_max = temp_series.max()\n",
        "    series_min = temp_series.min()\n",
        "    series_range = temp_series.max() - temp_series.min()\n",
        "\n",
        "    for j in range(temp_series.shape[0]):\n",
        "\n",
        "        no_duplicate_df.iat[j,i] = (no_duplicate_df.iat[j,i]-series_min)/series_range\n",
        "\n",
        "\n",
        "no_dupe_normalized_df = no_duplicate_df\n",
        "\n",
        "no_dupe_normalized_df.to_csv(\"PredicitiveMaintenanceDatasetPROCESSED.csv\")"
      ],
      "metadata": {
        "id": "keDopJK_YSRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Pre-Processing\n",
        "See sections below to get more details on how we all completed the advanced pre-processing of our datasets"
      ],
      "metadata": {
        "id": "NilDtXy85ceh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metro PT-3\n",
        "*Xianyu & Elijah*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Amelia's notes from when they were mistakenly trying to complete the advanced pre-processing of the Metro PT-3 dataset:**\n",
        "\n",
        "\n",
        "Advanced pre-processing focused on the following sub-question:\n",
        "*   How well do electrical load (Motor_current), temperature (Oil_temperature), and output pressure (TP2, TP3) predict compressor stress or early failure?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "30s0MCMV5sNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "# import dataset and stuff\n",
        "\n",
        "# make sure that simple pre-processed dataset is clean & normalised\n",
        "df = df.dropna()\n",
        "df = df.sort_values(\"Time Elapased [seconds]\")\n",
        "\n",
        "# scale numeric features for ordinal SVM\n",
        "features = [\"Motor_current\", \"Oil_temperature\", \"TP2\", \"TP3\", \"Reservoirs\"]\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# creating derived features\n",
        "df[\"Temp_x_current\"] = df[\"Oil_temperature\"] * df[\"Motor_current\"]              # thermal stress\n",
        "df[\"Pressure_ratio\"] = df[\"TP2\"] / (df[\"TP3\"] + 1e-6)                           # compressor efficiency\n",
        "df[\"Reservoir_diff\"] = df[\"Reservoirs\"].diff().fillna(0)                        # system stability over time\n",
        "df[\"Motor_delta\"] = df[\"Motor_current\"].diff().fillna(0)                        # system stability over time (pt2: electric boogaloo)\n",
        "\n",
        "# ordinal target variable\n",
        "df[\"HealthState\"] = np.select([(df[\"Oil_temperature\"] < 0.4) & (df[\"Motor_current\"] < 0.4), (df[\"Oil_temperature\"] < 0.7) & (df[\"Motor_current\"] < 0.7),],[0, 1],default=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "reyNZ30D6A2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NASA\n",
        "*Xianyu & Amelia *\n",
        "\n",
        "Advanced preprocessing involved..."
      ],
      "metadata": {
        "id": "6RzE4rLxpyAl"
      }
    }
  ]
}