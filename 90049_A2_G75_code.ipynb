{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosfet-sudo/ML-Project-G75/blob/main/90049_A2_G75_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COMP90049 (IML) → A2 GROUP 75 PROJECT CODE**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Elijah Cullinan, 1352251, ecullinan@student.unimelb.edu.au\n",
        "\n",
        "Amelia King, 1175861, kingal@student.unimelb.edu.au\n",
        "\n",
        "Xinyu Xu, 1508869, xinyu11@student.unimelb.edu.au\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   Linked to (public) GitHub *mosfet-sudo/ML-Project-G75*\n",
        "    * *Note → GitHub must be public in order for us to view the 'live updates' of any changes made, as well as to more easily do common File I/O operations*\n",
        "*   List item\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Reading appropriate files into this Notebook\n",
        "In order for this shared notebook to work, files must be shared & filepaths must be generic to that file sharing method. There are two options for this; option 1 should be the main method, use option2 if option 1 doesn't work for whatever reason.\n",
        "\n",
        "Code blocks will need to be adjusted according to whatever method being used (wherever the files are being read from as well as where output files are saved), and both the repository&drive must be updated so that they reflect all the changes.\n",
        "\n",
        "### Option 1: Read directly from *mosfet-sudo/ML-Project-G75*\n",
        "Run the appropriate code block below for the Colab notebook to read files directly from the\n",
        "\n",
        "### Option 2: Mount Google Drive\n",
        "\n",
        "Run the appropriate code block below to mount your google drive to this colab notebook for however long you need to be running the code for.\n",
        "\n",
        "*NOTE: You must accept Amelia's invitation to share the drive folder 'IML_G75_Colab_Notebooks in order for this to work as intended.*\n"
      ],
      "metadata": {
        "id": "m5JmuJbLWmzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTION 1: READ FROM GITHUB\n",
        "!git clone https://github.com/mosfet-sudo/ML-Project-G75\n",
        "\n"
      ],
      "metadata": {
        "id": "Ri4j_coqwPal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTION 2: MOUNT GOOGLE DRIVE\n",
        "from google.colab import drive # To read in data from google drive (delete later?)\n",
        "drive.mount('/content/drive') # to read in data from google drive (delete later?)\n",
        "\n",
        "# ------\n",
        "# '/content/drive/My Drive/IML_G75_Colab_Notebooks/...'"
      ],
      "metadata": {
        "id": "Ru_ouPJqYHq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c547e63-3c5e-45d0-dae9-320eaa1b6684"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simple Data Processing**\n",
        "*Elijah*\n",
        "\n",
        "Each Dataset was checked for duplicate entries, and then specific numeric values were normalized. In addition to this there was some minor text processing of the NASA dataset where the text files were tranferred into csv format. Furthermore adjustments were made to the metrodataset where all data points of each minute were averaged in order to make the dataset smaller. In particular for values that were binary (i.e) 1 or 0 if the average was strictly greater than 0.5 then it was set to 1 otherwise it was set to 0. This averaging should reduce noise and reduce the affect of anomalous outliers."
      ],
      "metadata": {
        "id": "-glSLfYeXa2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metro-PT3\n",
        "The raw data file is very large; seeing as Elijah has already completed the simple pre-processing it would be advisable not to run this code block again."
      ],
      "metadata": {
        "id": "fILNNgYJYIM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\"\"\" Averaging function for new dataset that preserves when errors occur, n must\n",
        "be even n=6 for averaging every minute. Takes the data frame, the number of\n",
        "samples to bunch together and average and the indexes that are we are applying\n",
        "the average of. The inputs are the data frame, an integer n, and 2 arrays of\n",
        "numeric indexes. Returns the new (smaller) averaged dataframe. \"\"\"\n",
        "# above copied from below documentation; will need to be cleaned up bc half those sentences don't make no sense\n",
        "\n",
        "## averaging function for new dataset that preserves when errors occur, n must be even n=6 for averaging every minute\n",
        "## takes the data  frame the number of samples to bunch together and average and the indexes that are we are applying the averaging off.\n",
        "## the inputs are the data frame, an integer n, and 2 arrays of numeric indexes it returns the new (smaller) averaged dataframe\n",
        "\n",
        "def timeAvg(df, n, tempindexes, elec_temp_indexes):\n",
        "    m = n/2\n",
        "    m = int(m)\n",
        "\n",
        "    df_empty = df.copy(deep=True).iloc[0:0]\n",
        "    avg_atpoints = []\n",
        "    max_atpoints = []\n",
        "    for i in tempindexes:\n",
        "        print(i)\n",
        "        avg_atpoints = []\n",
        "        temp_series = df.iloc[:,i]\n",
        "        numrows = int(temp_series.shape[0])\n",
        "        for j in range(m, int(numrows-m),n):\n",
        "            avg_atpoints.append(temp_series.iloc[j-m:j+m].mean()) #here we take the average\n",
        "        df_empty.iloc[:,i] = avg_atpoints\n",
        "    for i in elec_temp_indexes:\n",
        "        print(i)\n",
        "        max_atpoints = []\n",
        "        temp_series = df.iloc[:,i]\n",
        "        numrows = int(temp_series.shape[0])\n",
        "        for j in range(m, int(numrows-m),n):\n",
        "            avg = temp_series.iloc[j-m:j+m].mean()  #we use the average to decide what to assign to this datapoint\n",
        "            if avg > 0.5:\n",
        "                max_atpoints.append(1)\n",
        "            else:\n",
        "                max_atpoints.append(0)\n",
        "        df_empty.iloc[:,i] = max_atpoints\n",
        "    df_empty.iloc[:,0] = list(range(1,df_empty.shape[0]+1))\n",
        "    return df_empty\n",
        "\n",
        "\n",
        "# df = pd.read_csv(\"C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/MetroPT3(AirCompressor).csv\") -> DELETE (Elijah)\n",
        "# df = pd.read_csv('') # OPTION1: GITHUB\n",
        "df = pd.read_csv('/content/drive/My Drive/IML_G75_Colab_Notebooks/') # OPTION2: GOOGLE DRIVE\n",
        "\n",
        "\n",
        "#Duplicate checking\n",
        "df = df.drop(columns = [\"timestamp\"])\n",
        "num_columns = df.shape[1]\n",
        "num_rows = df.shape[0]\n",
        "duplicateddf=df.duplicated()\n",
        "no_duplicate_df = df.loc[duplicateddf==False]\n",
        "\n",
        "\n",
        "no_duplicate_df.rename(columns={\"Unnamed: 0\": \"Time Elapased [seconds]\"}, inplace=True)\n",
        "print(no_duplicate_df.columns)\n",
        "\n",
        "# Normalisation\n",
        "# Find the indexes of columns which have values we want to normalize\n",
        "temp_indexes = [no_duplicate_df.columns.get_loc(\"Time Elapased [seconds]\")] ##SHOULD WE NORMALIsE THIS?\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"TP2\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"TP3\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"H1\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"DV_pressure\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Reservoirs\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Oil_temperature\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Motor_current\"))\n",
        "\n",
        "\n",
        "# BOTH RPM AND TOOLWEAR HAVE ISSUES WITH CERTAIN DTYPES BEING INT DUE TO WHOLE NUMBERS IN DATA\n",
        "no_duplicate_df[\"Time Elapased [seconds]\"] = no_duplicate_df[\"Time Elapased [seconds]\"].astype(float)\n",
        "\n",
        "#for each index normalize the values and for each row set the value to the normalized value\n",
        "for i in temp_indexes:\n",
        "    temp_series = no_duplicate_df.iloc[:,i]\n",
        "    series_max = temp_series.max()\n",
        "    series_min = temp_series.min()\n",
        "    series_range = temp_series.max() - temp_series.min()\n",
        "\n",
        "    for j in range(temp_series.shape[0]):\n",
        "\n",
        "        no_duplicate_df.iat[j,i] = (no_duplicate_df.iat[j,i]-series_min)/series_range\n",
        "\n",
        "\n",
        "\n",
        "## new temp indexes for finding columns of electrical signal data\n",
        "elec_temp_indexes = [no_duplicate_df.columns.get_loc(\"COMP\")]\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"DV_eletric\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Towers\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"MPG\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"LPS\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Pressure_switch\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Oil_level\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Caudal_impulses\"))\n",
        "\n",
        "\n",
        "\n",
        "no_dupe_normalized_df = no_duplicate_df\n",
        "no_dupe_normalized_df_avg = timeAvg(no_dupe_normalized_df,6,temp_indexes,elec_temp_indexes)\n",
        "\n",
        "no_dupe_normalized_df_avg.to_csv(\"METROPT3_AVGMINUTEPROCESSED.csv\")"
      ],
      "metadata": {
        "id": "-JPJp2XBYLL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NASA"
      ],
      "metadata": {
        "id": "iXTnSmEoYMNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Method for parsing text files into dataframe, takes a string input and returns a dataframe object\n",
        "def parseTxt2Dataframe(String):\n",
        "    with open(String, 'r', encoding='utf-8') as file:\n",
        "        lines = []\n",
        "        for line in file:\n",
        "            newline = line.strip()\n",
        "            newline = newline.split()\n",
        "            lines.append(newline)\n",
        "        new_df = pd.DataFrame(lines, columns=['Unit Number', 'Time in cycles',\n",
        "                                  'Operational Setting 1', 'Operational Setting 2','Operational Setting 3',\n",
        "                                  'T2','T24','T30','T50','P2','P15','P30','Nf','Nc','epr','Ps30','phi','NRf','NRc',\n",
        "                                  'BPR','farB','htBleed','Nf_dmd','PCNfr_dmd','W31','W32'])\n",
        "        num_cols = new_df.shape[1]\n",
        "        for i in range(num_cols):\n",
        "            new_df.iloc[:,i] = pd.to_numeric(new_df.iloc[:,i])\n",
        "\n",
        "\n",
        "    return new_df\n",
        "\n",
        "#normalizes the data, and checks for duplicates, takes dataframe object as input and returns a dataframe\n",
        "def normalize_and_checkdupe(df):\n",
        "    checkdf = df.drop(columns=['Unit Number','Time in cycles'])\n",
        "    duplicateddf = checkdf.duplicated()\n",
        "    no_dupe_df = df.loc[duplicateddf==False]\n",
        "    num_cols = no_dupe_df.shape[1]\n",
        "    num_rows = no_dupe_df.shape[0]\n",
        "    for i in range(2,num_cols):\n",
        "        max_val = no_dupe_df.iloc[:,i].max()\n",
        "        min_val = no_dupe_df.iloc[:,i].min()\n",
        "        val_range = max_val - min_val\n",
        "        for j in range(num_rows):\n",
        "            if val_range != 0:\n",
        "                no_dupe_df.iat[j, i] = (no_dupe_df.iat[j, i] - min_val) / val_range\n",
        "            else:\n",
        "                no_dupe_df.iat[j, i] = -1\n",
        "\n",
        "    return no_dupe_df\n",
        "\n",
        "# parsing each text file into a dataframe -> OPTION 1: GITHUB\n",
        "\n",
        "# parsing each text file into a dataframe -> OPTION 2: GOOGLE DRIVE\n",
        "df_test1 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD001.txt')\n",
        "df_test2 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD002.txt')\n",
        "df_test3 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD003.txt')\n",
        "df_test4 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD004.txt')\n",
        "df_train1 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD001.txt')\n",
        "df_train2 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD002.txt')\n",
        "df_train3 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD003.txt')\n",
        "df_train4 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD004.txt')\n",
        "\n",
        "\n",
        "\n",
        "#combining the data into one dataset\n",
        "\n",
        "df_all = pd.concat([df_test1,df_test2,df_test3,df_test4,df_train1,df_train2,df_train3,df_train4],ignore_index=True)\n",
        "print(df_all.shape)\n",
        "print(df_test1.shape)\n",
        "\n",
        "## normalizing and checking for duplicates when combined\n",
        "df_all_processed = normalize_and_checkdupe(df_all)\n",
        "\n",
        "## normalizing and checking for duplicates when uncombined\n",
        "df_test1_processed = normalize_and_checkdupe(df_test1)\n",
        "#df_test2_processed = normalize_and_checkdupe(df_test2)\n",
        "#df_test3_processed = normalize_and_checkdupe(df_test3)\n",
        "#df_test4_processed = normalize_and_checkdupe(df_test4)\n",
        "#df_train1_processed = normalize_and_checkdupe(df_train1)\n",
        "#df_train2_processed = normalize_and_checkdupe(df_train2)\n",
        "#df_train3_processed = normalize_and_checkdupe(df_train3)\n",
        "#df_train4_processed = normalize_and_checkdupe(df_train4)\n",
        "\n",
        "\n",
        "\n",
        "## testing and debug\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(df_all_processed.shape)\n",
        "print(df_all_processed.head())\n",
        "print(df_test1_processed.head())\n",
        "print(\"newline\")\n",
        "\n",
        "\n",
        "print(\"newline\")\n",
        "print(df_all_processed.shape)\n",
        "\n",
        "pd.reset_option('display.max_columns')\n",
        "df_all_processed.to_csv('NASAprocessed.csv')"
      ],
      "metadata": {
        "id": "-XhCb94XYPf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MScFRuDoWV4a",
        "outputId": "1cd8f771-c840-46bc-9cc9-876e4cc4e152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PdM"
      ],
      "metadata": {
        "id": "NHgRsQ1jYP9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# df = pd.read_csv(\"C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/ai_2020.csv\") DELETE -> Elijah\n",
        "# df = pd.read_csv('') # OPTION1: GITHUB\n",
        "df = pd.read_csv('/content/drive/My Drive/IML_G75_Colab_Notebooks/ai_2020.csv') # OPTION2: GOOGLE DRIVE\n",
        "\n",
        "#Duplicate checking\n",
        "\n",
        "df = df.drop(columns = [\"UDI\"])\n",
        "num_columns = df.shape[1]\n",
        "num_rows = df.shape[0]\n",
        "duplicateddf=df.duplicated()\n",
        "no_duplicate_df = df.loc[duplicateddf==False]\n",
        "\n",
        "print(no_duplicate_df.columns)\n",
        "#Normalization\n",
        "\n",
        "temp_indexes = [no_duplicate_df.columns.get_loc(\"Air temperature [K]\")]\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Process temperature [K]\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Rotational speed [rpm]\")) #TURN INTO SECONDS?\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Torque [Nm]\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Tool wear [min]\"))     # TURN INTO SECONDS?\n",
        "# BOTH RPM AND TOOLWEAR HAVE ISSUES WITH CERTAIN DTYPES BEING INT DUE TO WHOLE NUMBERS IN DATA\n",
        "no_duplicate_df[\"Rotational speed [rpm]\"] = no_duplicate_df[\"Rotational speed [rpm]\"].astype(float)\n",
        "no_duplicate_df[\"Tool wear [min]\"] = no_duplicate_df[\"Tool wear [min]\"].astype(float)\n",
        "\n",
        "for i in temp_indexes:\n",
        "\n",
        "    temp_series = no_duplicate_df.iloc[:,i]\n",
        "    series_max = temp_series.max()\n",
        "    series_min = temp_series.min()\n",
        "    series_range = temp_series.max() - temp_series.min()\n",
        "\n",
        "    for j in range(temp_series.shape[0]):\n",
        "\n",
        "        no_duplicate_df.iat[j,i] = (no_duplicate_df.iat[j,i]-series_min)/series_range\n",
        "\n",
        "\n",
        "no_dupe_normalized_df = no_duplicate_df\n",
        "\n",
        "no_dupe_normalized_df.to_csv(\"PredicitiveMaintenanceDatasetPROCESSED.csv\")"
      ],
      "metadata": {
        "id": "keDopJK_YSRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advanced Pre-Processing**\n",
        "*Xinyu, Elijah & Amelia*\n",
        "\n",
        "See sections below to get more details on how we all completed the advanced pre-processing of our datasets.\n",
        "\n",
        "*Note → around this point, we decided that we would likely not be using the PdM dataset any further.*"
      ],
      "metadata": {
        "id": "NilDtXy85ceh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metro PT-3\n",
        "*Xinyu & Elijah*\n",
        "\n",
        "[insert notes on what processing methods were used and why, here]\n",
        "\n",
        "* Majority class ensemble: want models that are better than random guessing\n",
        "\n",
        "* Greater standard deviations should have more weights\n",
        "* Whenever peak detected, ...\n",
        "* For more extreme/large events, cumulatively add score\n",
        "  * Issue: maintenance done on machine\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "30s0MCMV5sNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [insert correct code here] #"
      ],
      "metadata": {
        "id": "KSItJpvg1eNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Amelia's notes on Metro PT-3 advanced pre-processing\n",
        "*Mixed up which dataset I was meant to process, please enjoy my notes*\n",
        "\n",
        "Advanced pre-processing focused on the following sub-question:\n",
        "*   How well do electrical load (Motor_current), temperature (Oil_temperature), and output pressure (TP2, TP3) predict compressor stress or early failure?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Elijah Notes on Amelia's code**\n",
        "\n",
        "Looking at *causes;* want things to be generealisable\n",
        "\n",
        "`df[\"Temp_x_current\"] = df[\"Oil_temperature\"] * df[\"Motor_current\"]              # thermal stress`\n",
        "\n",
        "* Want features to be linearly related\n",
        "  * allows to more directly pinpoint what is causing failure (e.g., failure caused by overheating)\n",
        "* Absolute values VS ratio (dimensional analysis)\n",
        "\n",
        "`df[\"Motor_delta\"] = df[\"Motor_current\"].diff().fillna(0)                        # system stability over time (pt2)`\n",
        "\n",
        "* Actual current matters; high current burns wires, for example\n",
        "* Want to look at ratio of the current *now* over the most common (recent) operation range → how far away are these values from one another in terms of a ratio? (maybe make combined score with absolute value to weight both)\n",
        "   * More focus on generealisability"
      ],
      "metadata": {
        "id": "eaveTtEU1Cqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Amelia's mistake code ###\n",
        "# ---------------------------------------------------------------------------- #\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "# import dataset and stuff\n",
        "\n",
        "# make sure that simple pre-processed dataset is clean & normalised\n",
        "df = df.dropna()\n",
        "df = df.sort_values(\"Time Elapased [seconds]\")\n",
        "\n",
        "# scale numeric features for ordinal SVM\n",
        "features = [\"Motor_current\", \"Oil_temperature\", \"TP2\", \"TP3\", \"Reservoirs\"]\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# creating derived features\n",
        "df[\"Temp_x_current\"] = df[\"Oil_temperature\"] * df[\"Motor_current\"]              # thermal stress => MARK FOR CHANGE\n",
        "df[\"Pressure_ratio\"] = df[\"TP2\"] / (df[\"TP3\"] + 1e-6)                           # compressor efficiency\n",
        "df[\"Reservoir_diff\"] = df[\"Reservoirs\"].diff().fillna(0)                        # system stability over time => MARK FOR CHANGE: NORMALISE?\n",
        "df[\"Motor_delta\"] = df[\"Motor_current\"].diff().fillna(0)                        # system stability over time (pt2: electric boogaloo) => MARK FOR DELETION\n",
        "\n",
        "# ordinal target variable\n",
        "df[\"State_of_health\"] = np.select([(df[\"Oil_temperature\"] < 0.4) & (df[\"Motor_current\"] < 0.4), (df[\"Oil_temperature\"] < 0.7) & (df[\"Motor_current\"] < 0.7),],[0, 1],default=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "reyNZ30D6A2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NASA\n",
        "*Xinyu & Amelia *\n",
        "\n",
        "Advanced preprocessing involved...\n",
        "J\n",
        "\n",
        "Jasmine\n",
        "\n",
        "* divided test set & training set into different data frames\n",
        "*"
      ],
      "metadata": {
        "id": "6RzE4rLxpyAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [insert correct code here] #"
      ],
      "metadata": {
        "id": "LOtgyuR313Rj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}