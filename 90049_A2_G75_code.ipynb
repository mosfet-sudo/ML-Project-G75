{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO66GmWY1kXskgnAL3cr9wZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosfet-sudo/ML-Project-G75/blob/main/90049_A2_G75_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "note: the github must be public in order for us to view 'live updates' of any changes made to this notebook"
      ],
      "metadata": {
        "id": "m5JmuJbLWmzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Data Processing\n",
        "(Done by Elijah)\n",
        "{insert brief outline of what the simple pre-processing involved}"
      ],
      "metadata": {
        "id": "-glSLfYeXa2D"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ru_ouPJqYHq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metro-PT3"
      ],
      "metadata": {
        "id": "fILNNgYJYIM4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-JPJp2XBYLL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NASA"
      ],
      "metadata": {
        "id": "iXTnSmEoYMNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Method for parsing text files into dataframe, takes a string input and returns a dataframe object\n",
        "def parseTxt2Dataframe(String):\n",
        "    with open(String, 'r', encoding='utf-8') as file:\n",
        "        lines = []\n",
        "        for line in file:\n",
        "            newline = line.strip()\n",
        "            newline = newline.split()\n",
        "            lines.append(newline)\n",
        "        new_df = pd.DataFrame(lines, columns=['Unit Number', 'Time in cycles',\n",
        "                                  'Operational Setting 1', 'Operational Setting 2','Operational Setting 3',\n",
        "                                  'T2','T24','T30','T50','P2','P15','P30','Nf','Nc','epr','Ps30','phi','NRf','NRc',\n",
        "                                  'BPR','farB','htBleed','Nf_dmd','PCNfr_dmd','W31','W32'])\n",
        "        num_cols = new_df.shape[1]\n",
        "        for i in range(num_cols):\n",
        "            new_df.iloc[:,i] = pd.to_numeric(new_df.iloc[:,i])\n",
        "\n",
        "\n",
        "    return new_df\n",
        "\n",
        "#normalizes the data, and checks for duplicates, takes dataframe object as input and returns a dataframe\n",
        "def normalize_and_checkdupe(df):\n",
        "    checkdf = df.drop(columns=['Unit Number','Time in cycles'])\n",
        "    duplicateddf = checkdf.duplicated()\n",
        "    no_dupe_df = df.loc[duplicateddf==False]\n",
        "    num_cols = no_dupe_df.shape[1]\n",
        "    num_rows = no_dupe_df.shape[0]\n",
        "    for i in range(2,num_cols):\n",
        "        max_val = no_dupe_df.iloc[:,i].max()\n",
        "        min_val = no_dupe_df.iloc[:,i].min()\n",
        "        val_range = max_val - min_val\n",
        "        for j in range(num_rows):\n",
        "            if val_range != 0:\n",
        "                no_dupe_df.iat[j, i] = (no_dupe_df.iat[j, i] - min_val) / val_range\n",
        "            else:\n",
        "                no_dupe_df.iat[j, i] = -1\n",
        "\n",
        "    return no_dupe_df\n",
        "\n",
        "\n",
        "# parsing each text file into a dataframe\n",
        "df_test1 = parseTxt2Dataframe('C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/CMaps/test_FD001.txt')\n",
        "df_test2 = parseTxt2Dataframe('C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/CMaps/test_FD002.txt')\n",
        "df_test3 = parseTxt2Dataframe('C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/CMaps/test_FD003.txt')\n",
        "df_test4 = parseTxt2Dataframe('C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/CMaps/test_FD004.txt')\n",
        "df_train1 = parseTxt2Dataframe('C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/CMaps/train_FD001.txt')\n",
        "df_train2 = parseTxt2Dataframe('C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/CMaps/train_FD002.txt')\n",
        "df_train3 = parseTxt2Dataframe('C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/CMaps/train_FD003.txt')\n",
        "df_train4 = parseTxt2Dataframe('C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/CMaps/train_FD004.txt')\n",
        "\n",
        "\n",
        "\n",
        "#combining the data into one dataset\n",
        "\n",
        "df_all = pd.concat([df_test1,df_test2,df_test3,df_test4,df_train1,df_train2,df_train3,df_train4],ignore_index=True)\n",
        "print(df_all.shape)\n",
        "print(df_test1.shape)\n",
        "\n",
        "## normalizing and checking for duplicates when combined\n",
        "df_all_processed = normalize_and_checkdupe(df_all)\n",
        "\n",
        "## normalizing and checking for duplicates when uncombined\n",
        "df_test1_processed = normalize_and_checkdupe(df_test1)\n",
        "#df_test2_processed = normalize_and_checkdupe(df_test2)\n",
        "#df_test3_processed = normalize_and_checkdupe(df_test3)\n",
        "#df_test4_processed = normalize_and_checkdupe(df_test4)\n",
        "#df_train1_processed = normalize_and_checkdupe(df_train1)\n",
        "#df_train2_processed = normalize_and_checkdupe(df_train2)\n",
        "#df_train3_processed = normalize_and_checkdupe(df_train3)\n",
        "#df_train4_processed = normalize_and_checkdupe(df_train4)\n",
        "\n",
        "\n",
        "\n",
        "## testing and debug\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(df_all_processed.shape)\n",
        "print(df_all_processed.head())\n",
        "print(df_test1_processed.head())\n",
        "print(\"newline\")\n",
        "\n",
        "\n",
        "print(\"newline\")\n",
        "print(df_all_processed.shape)\n",
        "\n",
        "pd.reset_option('display.max_columns')\n",
        "df_all_processed.to_csv('NASAprocessed.csv')"
      ],
      "metadata": {
        "id": "-XhCb94XYPf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PdM"
      ],
      "metadata": {
        "id": "NHgRsQ1jYP9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "df = pd.read_csv(\"C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/ai_2020.csv\")\n",
        "#Duplicate checking\n",
        "\n",
        "df = df.drop(columns = [\"UDI\"])\n",
        "num_columns = df.shape[1]\n",
        "num_rows = df.shape[0]\n",
        "duplicateddf=df.duplicated()\n",
        "no_duplicate_df = df.loc[duplicateddf==False]\n",
        "\n",
        "print(no_duplicate_df.columns)\n",
        "#Normalization\n",
        "\n",
        "temp_indexes = [no_duplicate_df.columns.get_loc(\"Air temperature [K]\")]\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Process temperature [K]\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Rotational speed [rpm]\")) #TURN INTO SECONDS?\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Torque [Nm]\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Tool wear [min]\"))     # TURN INTO SECONDS?\n",
        "# BOTH RPM AND TOOLWEAR HAVE ISSUES WITH CERTAIN DTYPES BEING INT DUE TO WHOLE NUMBERS IN DATA\n",
        "no_duplicate_df[\"Rotational speed [rpm]\"] = no_duplicate_df[\"Rotational speed [rpm]\"].astype(float)\n",
        "no_duplicate_df[\"Tool wear [min]\"] = no_duplicate_df[\"Tool wear [min]\"].astype(float)\n",
        "\n",
        "for i in temp_indexes:\n",
        "\n",
        "    temp_series = no_duplicate_df.iloc[:,i]\n",
        "    series_max = temp_series.max()\n",
        "    series_min = temp_series.min()\n",
        "    series_range = temp_series.max() - temp_series.min()\n",
        "\n",
        "    for j in range(temp_series.shape[0]):\n",
        "\n",
        "        no_duplicate_df.iat[j,i] = (no_duplicate_df.iat[j,i]-series_min)/series_range\n",
        "\n",
        "\n",
        "no_dupe_normalized_df = no_duplicate_df\n",
        "\n",
        "no_dupe_normalized_df.to_csv(\"PredicitiveMaintenanceDatasetPROCESSED.csv\")"
      ],
      "metadata": {
        "id": "keDopJK_YSRR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}