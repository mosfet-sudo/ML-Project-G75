{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosfet-sudo/ML-Project-G75/blob/main/90049_A2_G75_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COMP90049 (IML) → A2 GROUP 75 PROJECT CODE**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Elijah Cullinan, 1352251, ecullinan@student.unimelb.edu.au\n",
        "\n",
        "Amelia King, 1175861, kingal@student.unimelb.edu.au\n",
        "\n",
        "Xinyu Xu, 1508869, xinyu11@student.unimelb.edu.au\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   Linked to (public) GitHub *mosfet-sudo/ML-Project-G75*\n",
        "    * *Note → GitHub must be public in order for us to view the 'live updates' of any changes made, as well as to more easily do common File I/O operations*\n",
        "*   List item\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Reading appropriate files into this Notebook\n",
        "In order for this shared notebook to work, files must be shared & filepaths must be generic to that file sharing method. There are two options for this; option 1 should be the main method, use option2 if option 1 doesn't work for whatever reason.\n",
        "\n",
        "Code blocks will need to be adjusted according to whatever method being used (wherever the files are being read from as well as where output files are saved), and both the repository&drive must be updated so that they reflect all the changes.\n",
        "\n",
        "### Option 1: Read directly from *mosfet-sudo/ML-Project-G75*\n",
        "Run the appropriate code block below for the Colab notebook to read files directly from the\n",
        "\n",
        "### Option 2: Mount Google Drive\n",
        "\n",
        "Run the appropriate code block below to mount your google drive to this colab notebook for however long you need to be running the code for.\n",
        "\n",
        "*NOTE: You must accept Amelia's invitation to share the drive folder 'IML_G75_Colab_Notebooks in order for this to work as intended.*\n"
      ],
      "metadata": {
        "id": "m5JmuJbLWmzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTION 1: READ FROM GITHUB\n",
        "!git clone https://github.com/mosfet-sudo/ML-Project-G75\n",
        "\n"
      ],
      "metadata": {
        "id": "Ri4j_coqwPal",
        "outputId": "e221b451-df48-4ff3-d07b-6353c2e632eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ML-Project-G75' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTION 2: MOUNT GOOGLE DRIVE\n",
        "from google.colab import drive # To read in data from google drive (delete later?)\n",
        "drive.mount('/content/drive') # to read in data from google drive (delete later?)\n",
        "\n",
        "# ------\n",
        "# '/content/drive/My Drive/IML_G75_Colab_Notebooks/...'"
      ],
      "metadata": {
        "id": "Ru_ouPJqYHq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5203fb-6b83-4d94-cd6d-eaad6a639623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/IML_G75_Colab_Notebooks/\n"
      ],
      "metadata": {
        "id": "n47-n52jsUvI",
        "outputId": "94a4851e-852a-4006-ec49-cff1887a00da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CMaps_NASA\n",
            "'COMP90049 G75 RESEARCH PROJECT NOTES.gdoc'\n",
            "'IML_G75_Copy_(EMPTY).ipynb'\n",
            " METROPT3_AVGMINUTEPROCESSED.csv\n",
            " METROPT3_AVGMINUTEPROCESSED_WITHTIMES.csv\n",
            " NASAprocessed.csv\n",
            " pdm_2020.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simple Data Processing**\n",
        "*Elijah*\n",
        "\n",
        "Each Dataset was checked for duplicate entries, and then specific numeric values were normalized. In addition to this there was some minor text processing of the NASA dataset where the text files were tranferred into csv format. Furthermore adjustments were made to the metrodataset where all data points of each minute were averaged in order to make the dataset smaller. In particular for values that were binary (i.e) 1 or 0 if the average was strictly greater than 0.5 then it was set to 1 otherwise it was set to 0. This averaging should reduce noise and reduce the affect of anomalous outliers."
      ],
      "metadata": {
        "id": "-glSLfYeXa2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metro-PT3\n",
        "The raw data file is very large; seeing as Elijah has already completed the simple pre-processing it would be advisable not to run this code block again."
      ],
      "metadata": {
        "id": "fILNNgYJYIM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\"\"\" Averaging function for new dataset that preserves when errors occur, n must\n",
        "be even n=6 for averaging every minute. Takes the data frame, the number of\n",
        "samples to bunch together and average and the indexes that are we are applying\n",
        "the average of. The inputs are the data frame, an integer n, and 2 arrays of\n",
        "numeric indexes. Returns the new (smaller) averaged dataframe. \"\"\"\n",
        "# above copied from below documentation; will need to be cleaned up bc half those sentences don't make no sense\n",
        "\n",
        "## averaging function for new dataset that preserves when errors occur, n must be even n=6 for averaging every minute\n",
        "## takes the data  frame the number of samples to bunch together and average and the indexes that are we are applying the averaging off.\n",
        "## the inputs are the data frame, an integer n, and 2 arrays of numeric indexes it returns the new (smaller) averaged dataframe\n",
        "\n",
        "def timeAvg(df, n, tempindexes, elec_temp_indexes):\n",
        "    m = n/2\n",
        "    m = int(m)\n",
        "\n",
        "    df_empty = df.copy(deep=True).iloc[0:0]\n",
        "    avg_atpoints = []\n",
        "    max_atpoints = []\n",
        "    for i in tempindexes:\n",
        "        print(i)\n",
        "        avg_atpoints = []\n",
        "        temp_series = df.iloc[:,i]\n",
        "        numrows = int(temp_series.shape[0])\n",
        "        for j in range(m, int(numrows-m),n):\n",
        "            avg_atpoints.append(temp_series.iloc[j-m:j+m].mean()) #here we take the average\n",
        "        df_empty.iloc[:,i] = avg_atpoints\n",
        "    for i in elec_temp_indexes:\n",
        "        print(i)\n",
        "        max_atpoints = []\n",
        "        temp_series = df.iloc[:,i]\n",
        "        numrows = int(temp_series.shape[0])\n",
        "        for j in range(m, int(numrows-m),n):\n",
        "            avg = temp_series.iloc[j-m:j+m].mean()  #we use the average to decide what to assign to this datapoint\n",
        "            if avg > 0.5:\n",
        "                max_atpoints.append(1)\n",
        "            else:\n",
        "                max_atpoints.append(0)\n",
        "        df_empty.iloc[:,i] = max_atpoints\n",
        "    df_empty.iloc[:,0] = list(range(1,df_empty.shape[0]+1))\n",
        "    return df_empty\n",
        "\n",
        "\n",
        "# df = pd.read_csv(\"C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/MetroPT3(AirCompressor).csv\") -> DELETE (Elijah)\n",
        "# df = pd.read_csv('') # OPTION1: GITHUB\n",
        "df = pd.read_csv('/content/drive/My Drive/IML_G75_Colab_Notebooks/') # OPTION2: GOOGLE DRIVE\n",
        "\n",
        "\n",
        "\n",
        "#Duplicate checking\n",
        "df = df.drop(columns = [\"timestamp\"])\n",
        "num_columns = df.shape[1]\n",
        "num_rows = df.shape[0]\n",
        "duplicateddf=df.duplicated()\n",
        "no_duplicate_df = df.loc[duplicateddf==False]\n",
        "\n",
        "\n",
        "no_duplicate_df.rename(columns={\"Unnamed: 0\": \"Time Elapased [seconds]\"}, inplace=True)\n",
        "print(no_duplicate_df.columns)\n",
        "\n",
        "# Normalisation\n",
        "# Find the indexes of columns which have values we want to normalize\n",
        "temp_indexes = [no_duplicate_df.columns.get_loc(\"Time Elapased [seconds]\")] ##SHOULD WE NORMALIsE THIS?\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"TP2\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"TP3\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"H1\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"DV_pressure\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Reservoirs\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Oil_temperature\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Motor_current\"))\n",
        "\n",
        "\n",
        "# BOTH RPM AND TOOLWEAR HAVE ISSUES WITH CERTAIN DTYPES BEING INT DUE TO WHOLE NUMBERS IN DATA\n",
        "no_duplicate_df[\"Time Elapased [seconds]\"] = no_duplicate_df[\"Time Elapased [seconds]\"].astype(float)\n",
        "\n",
        "#for each index normalize the values and for each row set the value to the normalized value\n",
        "for i in temp_indexes:\n",
        "    temp_series = no_duplicate_df.iloc[:,i]\n",
        "    series_max = temp_series.max()\n",
        "    series_min = temp_series.min()\n",
        "    series_range = temp_series.max() - temp_series.min()\n",
        "\n",
        "    for j in range(temp_series.shape[0]):\n",
        "\n",
        "        no_duplicate_df.iat[j,i] = (no_duplicate_df.iat[j,i]-series_min)/series_range\n",
        "\n",
        "\n",
        "\n",
        "## new temp indexes for finding columns of electrical signal data\n",
        "elec_temp_indexes = [no_duplicate_df.columns.get_loc(\"COMP\")]\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"DV_eletric\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Towers\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"MPG\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"LPS\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Pressure_switch\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Oil_level\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Caudal_impulses\"))\n",
        "\n",
        "\n",
        "\n",
        "no_dupe_normalized_df = no_duplicate_df\n",
        "no_dupe_normalized_df_avg = timeAvg(no_dupe_normalized_df,6,temp_indexes,elec_temp_indexes)\n",
        "\n",
        "no_dupe_normalized_df_avg.to_csv(\"METROPT3_AVGMINUTEPROCESSED.csv\")"
      ],
      "metadata": {
        "id": "-JPJp2XBYLL7",
        "outputId": "7f3bc221-31fd-4fb5-88fa-cbb814125b43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: '/content/drive/MyDrive/IML_G75_Colab_Notebooks/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2322506135.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# df = pd.read_csv('/content/drive/My Drive/IML_G75_Colab_Notebooks/') # OPTION2: GOOGLE DRIVE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# please refer to your drive name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/IML_G75_Colab_Notebooks/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# OPTION2: GOOGLE DRIVE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/drive/MyDrive/IML_G75_Colab_Notebooks/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NASA"
      ],
      "metadata": {
        "id": "iXTnSmEoYMNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#Method for parsing text files into dataframe, takes a string input and returns a dataframe object\n",
        "def parseTxt2Dataframe(String):\n",
        "    with open(String, 'r', encoding='utf-8') as file:\n",
        "        lines = []\n",
        "        for line in file:\n",
        "            newline = line.strip()\n",
        "            newline = newline.split()\n",
        "            lines.append(newline)\n",
        "        new_df = pd.DataFrame(lines, columns=['Unit Number', 'Time in cycles',\n",
        "                                  'Operational Setting 1', 'Operational Setting 2','Operational Setting 3',\n",
        "                                  'T2','T24','T30','T50','P2','P15','P30','Nf','Nc','epr','Ps30','phi','NRf','NRc',\n",
        "                                  'BPR','farB','htBleed','Nf_dmd','PCNfr_dmd','W31','W32'])\n",
        "        num_cols = new_df.shape[1]\n",
        "        for i in range(num_cols):\n",
        "            new_df.iloc[:,i] = pd.to_numeric(new_df.iloc[:,i])\n",
        "\n",
        "\n",
        "    return new_df\n",
        "\n",
        "#normalizes the data, and checks for duplicates, takes dataframe object as input and returns a dataframe\n",
        "def normalize_and_checkdupe(df):\n",
        "    checkdf = df.drop(columns=['Unit Number','Time in cycles'])\n",
        "    duplicateddf = checkdf.duplicated()\n",
        "    no_dupe_df = df.loc[duplicateddf==False]\n",
        "    num_cols = no_dupe_df.shape[1]\n",
        "    num_rows = no_dupe_df.shape[0]\n",
        "    for i in range(2,num_cols):\n",
        "        max_val = no_dupe_df.iloc[:,i].max()\n",
        "        min_val = no_dupe_df.iloc[:,i].min()\n",
        "        val_range = max_val - min_val\n",
        "        for j in range(num_rows):\n",
        "            if val_range != 0:\n",
        "                no_dupe_df.iat[j, i] = (no_dupe_df.iat[j, i] - min_val) / val_range\n",
        "            else:\n",
        "                no_dupe_df.iat[j, i] = -1\n",
        "\n",
        "    return no_dupe_df\n",
        "\n",
        "# parsing each text file into a dataframe -> OPTION 1: GITHUB\n",
        "\n",
        "# parsing each text file into a dataframe -> OPTION 2: GOOGLE DRIVE\n",
        "df_test1 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD001.txt')\n",
        "df_test2 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD002.txt')\n",
        "df_test3 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD003.txt')\n",
        "df_test4 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD004.txt')\n",
        "df_train1 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD001.txt')\n",
        "df_train2 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD002.txt')\n",
        "df_train3 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD003.txt')\n",
        "df_train4 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD004.txt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#combining the data into one dataset\n",
        "\n",
        "df_all = pd.concat([df_test1,df_test2,df_test3,df_test4,df_train1,df_train2,df_train3,df_train4],ignore_index=True)\n",
        "print(df_all.shape)\n",
        "print(df_test1.shape)\n",
        "\n",
        "## normalizing and checking for duplicates when combined\n",
        "df_all_processed = normalize_and_checkdupe(df_all)\n",
        "\n",
        "## normalizing and checking for duplicates when uncombined\n",
        "df_test1_processed = normalize_and_checkdupe(df_test1)\n",
        "#df_test2_processed = normalize_and_checkdupe(df_test2)\n",
        "#df_test3_processed = normalize_and_checkdupe(df_test3)\n",
        "#df_test4_processed = normalize_and_checkdupe(df_test4)\n",
        "#df_train1_processed = normalize_and_checkdupe(df_train1)\n",
        "#df_train2_processed = normalize_and_checkdupe(df_train2)\n",
        "#df_train3_processed = normalize_and_checkdupe(df_train3)\n",
        "#df_train4_processed = normalize_and_checkdupe(df_train4)\n",
        "\n",
        "\n",
        "\n",
        "## testing and debug\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(df_all_processed.shape)\n",
        "print(df_all_processed.head())\n",
        "print(df_test1_processed.head())\n",
        "print(\"newline\")\n",
        "\n",
        "\n",
        "print(\"newline\")\n",
        "print(df_all_processed.shape)\n",
        "\n",
        "pd.reset_option('display.max_columns')\n",
        "df_all_processed.to_csv('NASAprocessed.csv')"
      ],
      "metadata": {
        "id": "-XhCb94XYPf8",
        "outputId": "16a5d46c-4173-4112-afe6-39ce4a49eb48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(265256, 26)\n",
            "(13096, 26)\n",
            "(265256, 26)\n",
            "  Unit Number Time in cycles Operational Setting 1 Operational Setting 2  \\\n",
            "0           1              1              0.000262              0.001068   \n",
            "1           1              2              0.000143              0.000356   \n",
            "2           1              3              0.000214              0.000831   \n",
            "3           1              4              0.000307              0.000712   \n",
            "4           1              5               0.00024              0.000712   \n",
            "\n",
            "  Operational Setting 3   T2       T24       T30       T50   P2  P15  \\\n",
            "0                   1.0  1.0  0.980948  0.915898   0.89639  1.0  1.0   \n",
            "1                   1.0  1.0  0.969006  0.924302  0.889711  1.0  1.0   \n",
            "2                   1.0  1.0  0.975843  0.920286  0.903883  1.0  1.0   \n",
            "3                   1.0  1.0  0.975661  0.912786  0.916044  1.0  1.0   \n",
            "4                   1.0  1.0  0.976299  0.920951  0.905271  1.0  1.0   \n",
            "\n",
            "        P30        Nf        Nc       epr    Ps30       phi       NRf  \\\n",
            "0  0.961104  0.998734  0.845708  0.948718  0.8936  0.961464  0.993222   \n",
            "1  0.963289  0.998671  0.849081  0.948718  0.9176  0.962539  0.993304   \n",
            "2  0.961587  0.998755  0.851097  0.948718  0.9176  0.962075  0.993222   \n",
            "3  0.961495  0.998713  0.841835  0.948718     0.9  0.960633  0.993277   \n",
            "4  0.961702  0.998671  0.841248  0.948718  0.9024  0.962515  0.993222   \n",
            "\n",
            "        NRc       BPR farB   htBleed Nf_dmd PCNfr_dmd       W31       W32  \n",
            "0   0.62457  0.085515  1.0  0.918367    1.0       1.0  0.965355  0.967837  \n",
            "1  0.655981   0.07696  1.0  0.928571    1.0       1.0  0.970737  0.968846  \n",
            "2  0.634728   0.09888  1.0  0.928571    1.0       1.0  0.972755   0.97024  \n",
            "3  0.640979  0.080877  1.0  0.908163    1.0       1.0  0.970064  0.967848  \n",
            "4  0.633478  0.084794  1.0  0.897959    1.0       1.0  0.969728  0.970039  \n",
            "  Unit Number Time in cycles Operational Setting 1 Operational Setting 2  \\\n",
            "0           1              1               0.65625              0.692308   \n",
            "1           1              2               0.34375              0.230769   \n",
            "2           1              3               0.53125              0.538462   \n",
            "3           1              4                 0.775              0.461538   \n",
            "4           1              5                   0.6              0.461538   \n",
            "\n",
            "  Operational Setting 3  T2       T24       T30       T50  P2  P15       P30  \\\n",
            "0                    -1  -1  0.596215  0.421968  0.282214  -1  1.0  0.608871   \n",
            "1                    -1  -1  0.182965  0.504025   0.22524  -1  1.0  0.800403   \n",
            "2                    -1  -1  0.419558  0.464814   0.34613  -1  1.0   0.65121   \n",
            "3                    -1  -1  0.413249  0.391587  0.449867  -1  1.0  0.643145   \n",
            "4                    -1  -1  0.435331  0.471306  0.357974  -1  1.0   0.66129   \n",
            "\n",
            "         Nf        Nc epr      Ps30       phi       NRf       NRc       BPR  \\\n",
            "0  0.365854  0.196475  -1  0.273973  0.534247  0.325581  0.152259  0.347076   \n",
            "1  0.292683  0.229042  -1  0.479452  0.634703  0.395349  0.277907  0.227709   \n",
            "2  0.390244  0.248506  -1  0.479452  0.591324  0.325581  0.192892  0.533557   \n",
            "3  0.341463   0.15908  -1  0.328767  0.456621  0.372093  0.217896  0.282359   \n",
            "4  0.292683   0.15341  -1  0.349315   0.63242  0.325581  0.187891  0.337009   \n",
            "\n",
            "  farB htBleed Nf_dmd PCNfr_dmd       W31       W32  \n",
            "0   -1   0.375     -1        -1       0.5  0.620099  \n",
            "1   -1     0.5     -1        -1  0.645455  0.645718  \n",
            "2   -1     0.5     -1        -1       0.7  0.681104  \n",
            "3   -1    0.25     -1        -1  0.627273  0.620382  \n",
            "4   -1   0.125     -1        -1  0.618182  0.676008  \n",
            "newline\n",
            "newline\n",
            "(265256, 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MScFRuDoWV4a",
        "outputId": "1cd8f771-c840-46bc-9cc9-876e4cc4e152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PdM"
      ],
      "metadata": {
        "id": "NHgRsQ1jYP9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# df = pd.read_csv(\"C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/ai_2020.csv\") DELETE -> Elijah\n",
        "# df = pd.read_csv('') # OPTION1: GITHUB\n",
        "df = pd.read_csv('/content/drive/My Drive/IML_G75_Colab_Notebooks/ai_2020.csv') # OPTION2: GOOGLE DRIVE\n",
        "\n",
        "#Duplicate checking\n",
        "\n",
        "df = df.drop(columns = [\"UDI\"])\n",
        "num_columns = df.shape[1]\n",
        "num_rows = df.shape[0]\n",
        "duplicateddf=df.duplicated()\n",
        "no_duplicate_df = df.loc[duplicateddf==False]\n",
        "\n",
        "print(no_duplicate_df.columns)\n",
        "#Normalization\n",
        "\n",
        "temp_indexes = [no_duplicate_df.columns.get_loc(\"Air temperature [K]\")]\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Process temperature [K]\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Rotational speed [rpm]\")) #TURN INTO SECONDS?\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Torque [Nm]\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Tool wear [min]\"))     # TURN INTO SECONDS?\n",
        "# BOTH RPM AND TOOLWEAR HAVE ISSUES WITH CERTAIN DTYPES BEING INT DUE TO WHOLE NUMBERS IN DATA\n",
        "no_duplicate_df[\"Rotational speed [rpm]\"] = no_duplicate_df[\"Rotational speed [rpm]\"].astype(float)\n",
        "no_duplicate_df[\"Tool wear [min]\"] = no_duplicate_df[\"Tool wear [min]\"].astype(float)\n",
        "\n",
        "for i in temp_indexes:\n",
        "\n",
        "    temp_series = no_duplicate_df.iloc[:,i]\n",
        "    series_max = temp_series.max()\n",
        "    series_min = temp_series.min()\n",
        "    series_range = temp_series.max() - temp_series.min()\n",
        "\n",
        "    for j in range(temp_series.shape[0]):\n",
        "\n",
        "        no_duplicate_df.iat[j,i] = (no_duplicate_df.iat[j,i]-series_min)/series_range\n",
        "\n",
        "\n",
        "no_dupe_normalized_df = no_duplicate_df\n",
        "\n",
        "no_dupe_normalized_df.to_csv(\"PredicitiveMaintenanceDatasetPROCESSED.csv\")"
      ],
      "metadata": {
        "id": "keDopJK_YSRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advanced Pre-Processing**\n",
        "*Xinyu, Elijah & Amelia*\n",
        "\n",
        "See sections below to get more details on how we all completed the advanced pre-processing of our datasets.\n",
        "\n",
        "*Note → around this point, we decided that we would likely not be using the PdM dataset any further.*"
      ],
      "metadata": {
        "id": "NilDtXy85ceh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metro PT-3\n",
        "*Xinyu & Elijah*\n",
        "\n",
        "[insert notes on what processing methods were used and why, here]\n",
        "\n",
        "* Majority class ensemble: want models that are better than random guessing\n",
        "\n",
        "* Greater standard deviations should have more weights\n",
        "* Whenever peak detected, ...\n",
        "* For more extreme/large events, cumulatively add score\n",
        "  * Issue: maintenance done on machine\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "30s0MCMV5sNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [insert correct code here] #"
      ],
      "metadata": {
        "id": "KSItJpvg1eNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Amelia's notes on Metro PT-3 advanced pre-processing\n",
        "*Mixed up which dataset I was meant to process, please enjoy my notes*\n",
        "\n",
        "Advanced pre-processing focused on the following sub-question:\n",
        "*   How well do electrical load (Motor_current), temperature (Oil_temperature), and output pressure (TP2, TP3) predict compressor stress or early failure?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Elijah Notes on Amelia's code**\n",
        "\n",
        "Looking at *causes;* want things to be generealisable\n",
        "\n",
        "`df[\"Temp_x_current\"] = df[\"Oil_temperature\"] * df[\"Motor_current\"]              # thermal stress`\n",
        "\n",
        "* Want features to be linearly related\n",
        "  * allows to more directly pinpoint what is causing failure (e.g., failure caused by overheating)\n",
        "* Absolute values VS ratio (dimensional analysis)\n",
        "\n",
        "`df[\"Motor_delta\"] = df[\"Motor_current\"].diff().fillna(0)                        # system stability over time (pt2)`\n",
        "\n",
        "* Actual current matters; high current burns wires, for example\n",
        "* Want to look at ratio of the current *now* over the most common (recent) operation range → how far away are these values from one another in terms of a ratio? (maybe make combined score with absolute value to weight both)\n",
        "   * More focus on generealisability"
      ],
      "metadata": {
        "id": "eaveTtEU1Cqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Amelia's mistake code ###\n",
        "# ---------------------------------------------------------------------------- #\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "# import dataset and stuff\n",
        "\n",
        "# make sure that simple pre-processed dataset is clean & normalised\n",
        "df = df.dropna()\n",
        "df = df.sort_values(\"Time Elapased [seconds]\")\n",
        "\n",
        "# scale numeric features for ordinal SVM\n",
        "features = [\"Motor_current\", \"Oil_temperature\", \"TP2\", \"TP3\", \"Reservoirs\"]\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "\n",
        "\n",
        "# creating derived features\n",
        "df[\"Temp_x_current\"] = df[\"Oil_temperature\"] * df[\"Motor_current\"]              # thermal stress => MARK FOR CHANGE\n",
        "df[\"Pressure_ratio\"] = df[\"TP2\"] / (df[\"TP3\"] + 1e-6)                           # compressor efficiency\n",
        "df[\"Reservoir_diff\"] = df[\"Reservoirs\"].diff().fillna(0)                        # system stability over time => MARK FOR CHANGE: NORMALISE?\n",
        "df[\"Motor_delta\"] = df[\"Motor_current\"].diff().fillna(0)                        # system stability over time (pt2: electric boogaloo) => MARK FOR DELETION\n",
        "\n",
        "# ordinal target variable\n",
        "df[\"State_of_health\"] = np.select([(df[\"Oil_temperature\"] < 0.4) & (df[\"Motor_current\"] < 0.4), (df[\"Oil_temperature\"] < 0.7) & (df[\"Motor_current\"] < 0.7),],[0, 1],default=2)\n",
        "\"\"\"\n",
        "### Elijah's code ###\n",
        "def addFailure(df, index):\n",
        "    failurecolindex = df.columns.get_loc('Failure')\n",
        "    for i in range(index[0], index[1]):\n",
        "        df.iat[i, failurecolindex] = 1\n",
        "\n",
        "df = pd.read_csv(\"C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/METROPT3_AVGMINUTEPROCESSED.csv\")\n",
        "\n",
        "failureindex1 = [93761,95204]\n",
        "failureindex2 = [140123,140517]\n",
        "failureindex3 = [148468,151354]\n",
        "failureindex4 = [195182,195453]\n",
        "df[\"Failure\"] = 0\n",
        "\n",
        "\n",
        "addFailure(df,failureindex1)\n",
        "addFailure(df,failureindex2)\n",
        "addFailure(df,failureindex3)\n",
        "addFailure(df,failureindex4)\n",
        "### CONSTRUCTING DAYS TO FAILURE TARGET FEATURE\n",
        "df[\"DaysToFailure\"] = 0\n",
        "D2Failurecolindex = df.columns.get_loc('DaysToFailure')\n",
        "referenceday1 = df[\"DaysSinceStart\"].iloc[failureindex1[0]]\n",
        "referenceday2 = df[\"DaysSinceStart\"].iloc[failureindex2[0]]\n",
        "referenceday3 = df[\"DaysSinceStart\"].iloc[failureindex3[0]]\n",
        "referenceday4 = df[\"DaysSinceStart\"].iloc[failureindex4[0]]\n",
        "referencedaylast = df[\"DaysSinceStart\"].iloc[-1]\n",
        "for i in range(0,failureindex1[0]):\n",
        "    df.iat[i, D2Failurecolindex] = referenceday1-df[\"DaysSinceStart\"].iloc[i]\n",
        "\n",
        "for i in range(failureindex1[1],failureindex2[0]):\n",
        "    df.iat[i, D2Failurecolindex] = referenceday2-df[\"DaysSinceStart\"].iloc[i]\n",
        "\n",
        "for i in range(failureindex2[1],failureindex3[0]):\n",
        "    df.iat[i, D2Failurecolindex] = referenceday3-df[\"DaysSinceStart\"].iloc[i]\n",
        "\n",
        "for i in range(failureindex3[1],failureindex4[0]):\n",
        "    df.iat[i, D2Failurecolindex] = referenceday4-df[\"DaysSinceStart\"].iloc[i]\n",
        "\n",
        "for i in range(failureindex4[1],len(df)):\n",
        "    df.iat[i, D2Failurecolindex] = referencedaylast - df[\"DaysSinceStart\"].iloc[i]\n",
        "#TRANFORM TO WEEKS IF NECESSARY\n",
        "#CREATING DIMENSIONLESS FEATURES\n",
        "\n",
        "power_temp_ratio = (df[\"Motor_current\"]*df[\"Motor_current\"])/(1e-6 + df[\"Oil_temperature\"]) #WOULD delta temp be better?\n",
        "reservior_flowrate = df[\"Reservoirs\"].diff().fillna(0)\n",
        "compressor_eff = df[\"TP2\"] / (df[\"TP3\"] + 1e-6)\n",
        "\n",
        "#flicker values that tracks when any electrical signals are flipping\n",
        "flicker_COMP = df[\"COMP\"].diff().fillna(0)\n",
        "flicker_DV = df[\"DV_eletric\"].diff().fillna(0)\n",
        "flicker_TOWER = df[\"Towers\"].diff().fillna(0)\n",
        "flicker_MPG = df[\"MPG\"].diff().fillna(0)\n",
        "flicker_LPS = df[\"LPS\"].diff().fillna(0)\n",
        "flicker_Pressure = df[\"Pressure_switch\"].diff().fillna(0)\n",
        "flicker_Oil = df[\"Oil_level\"].diff().fillna(0)\n",
        "flicker_CImpulse = df[\"Caudal_impulses\"].diff().fillna(0)\n",
        "\n",
        "# WANT to normalize each with respect to thier occurance and then add together\n",
        "def weight(flicker):\n",
        "    flicker = abs(flicker)\n",
        "\n",
        "    newflicker = flicker * (1/(sum(flicker)+1e-6))\n",
        "    return newflicker\n",
        "flicker_COMP = weight(flicker_COMP)\n",
        "flicker_DV =   weight(flicker_DV)\n",
        "flicker_TOWER = weight(flicker_TOWER)\n",
        "flicker_MPG = weight(flicker_MPG)\n",
        "flicker_LPS = weight(flicker_LPS)\n",
        "flicker_Pressure = weight(flicker_Pressure)\n",
        "flicker_Oil =  weight(flicker_Oil)\n",
        "flicker_CImpulse = weight(flicker_CImpulse)\n",
        "combined_flicker = flicker_COMP + flicker_DV + flicker_TOWER + flicker_MPG + flicker_LPS + flicker_Pressure + flicker_Oil + flicker_CImpulse\n",
        "\n",
        "data = {\"power_temp_ratio\":power_temp_ratio,\"reserviour_flowrate\":reservior_flowrate,\"compressor_eff\":compressor_eff, \"flicker\":combined_flicker}\n",
        "\n",
        "#FINDING AVERAGES FOR DIFFERENT RELATIVE TIMES WITH REGARD TO DIMENSIONLESS FEATURES\n",
        "# WILL GET A FAR AVERAGE (say 3 weeks ago ~ instances 20k-30k)\n",
        "def addFarStats(df,colIndex):\n",
        "    avgvals = []\n",
        "    sdvals = []\n",
        "    for i in range(30000,len(df)):\n",
        "        values = df.iloc[:,colIndex]\n",
        "\n",
        "        farvalues = values.iloc[i-30000:i-20000]\n",
        "\n",
        "        avg = farvalues.mean()\n",
        "        sd = farvalues.std()\n",
        "        avgvals.append(avg)\n",
        "        sdvals.append(sd)\n",
        "    df[f\"FarAvg{colIndex}\"] = 0.0\n",
        "    df[f\"FarSD{colIndex}\"] = 0.0\n",
        "    newcol1 = df.columns.get_loc(f'FarAvg{colIndex}')\n",
        "    newcol2 = df.columns.get_loc(f'FarSD{colIndex}')\n",
        "    for i in range(30000,len(df)):\n",
        "        df.iat[i, newcol1] = avgvals[i-30000]\n",
        "        df.iat[i, newcol2] = sdvals[i-30000]\n",
        "# OPTIONAL : GETS CLOSE STATISTICS\n",
        "def addCloseStats(df,colIndex):\n",
        "    avgvals = []\n",
        "    sdvals = []\n",
        "    for i in range(2000,len(df)):\n",
        "        values = df.iloc[:,colIndex]\n",
        "\n",
        "        farvalues = values.iloc[i-2000:i-1000]\n",
        "\n",
        "        avg = farvalues.mean()\n",
        "        sd = farvalues.std()\n",
        "        avgvals.append(avg)\n",
        "        sdvals.append(sd)\n",
        "    df[f\"CloseAvg{colIndex}\"] = 0.0\n",
        "    df[f\"CloseSD{colIndex}\"] = 0.0\n",
        "    newcol1 = df.columns.get_loc(f'CloseAvg{colIndex}')\n",
        "    newcol2 = df.columns.get_loc(f'CloseSD{colIndex}')\n",
        "    for i in range(2000,len(df)):\n",
        "        df.iat[i, newcol1] = avgvals[i-2000]\n",
        "        df.iat[i, newcol2] = sdvals[i-2000]\n",
        "\n",
        "\n",
        "# MAKE NEW DATAFRAME And calculate statistics\n",
        "newdf = pd.DataFrame(data)\n",
        "addFarStats(newdf,0)\n",
        "addFarStats(newdf,1)\n",
        "addFarStats(newdf,2)\n",
        "addFarStats(newdf,3)\n",
        "\n",
        "#import target features\n",
        "newdf[\"Failure\"] = df[\"Failure\"]\n",
        "newdf[\"Days2Failure\"] = df[\"DaysToFailure\"]\n",
        "\n",
        "newdf.to_csv(\"TESTING.csv\")\n",
        "\n",
        "\n",
        "#import scaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "### TAKE IN DATAFRAME\n",
        "df = pd.read_csv(\"C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/TESTING.csv\")\n",
        "### check for anomalous datapoints and construct series to determine which points are\n",
        "### uses previously calculated means and std's to find abnormal instances (CAN BE adjusted by changing the multiplier in the if else statements)\n",
        "def checkAnomalous(series1,seriesmeans,seriesstds):\n",
        "    checkseries = []\n",
        "    for i in range(len(series1)):\n",
        "        if seriesmeans.iloc[i] == 0 and seriesstds.iloc[i] == 0:\n",
        "            checkseries.append(0)\n",
        "        elif series1.iloc[i] > seriesmeans.iloc[i]+2*seriesstds.iloc[i]:\n",
        "            checkseries.append(1)\n",
        "        elif series1.iloc[i] < seriesmeans.iloc[i]-2*seriesstds.iloc[i]:\n",
        "            checkseries.append(1)\n",
        "        else:\n",
        "            checkseries.append(0)\n",
        "    return checkseries\n",
        "\n",
        "### takes cumulative values of last 10k instances of abnormal instances\n",
        "def recentCumulative(checkseries):\n",
        "    ## INITIAL CUMULATIVE\n",
        "    recentCumulative = []\n",
        "    recentCumulative = checkseries[0:10001].cumsum().tolist()\n",
        "    print(len(recentCumulative))\n",
        "    print(\"STARTING\")\n",
        "    for i in range(10001, len(checkseries)):\n",
        "        newentry = (recentCumulative[i-1]+checkseries[i]-checkseries[i-10000])\n",
        "        recentCumulative.append(newentry)\n",
        "    return recentCumulative\n",
        "# constructing new clean df and dropping unneeded features\n",
        "newdf = df\n",
        "newdf.drop(columns = ['Unnamed: 0'],inplace=True)\n",
        "\n",
        "#for each feature we find the cumulative values from last 10k instnaces\n",
        "for i in range(4):\n",
        "    print(i)\n",
        "\n",
        "    object = checkAnomalous(newdf.iloc[:,i],newdf[f\"FarAvg{i}\"],newdf[f\"FarSD{i}\"])\n",
        "    newdf[f\"checkseries{i}\"] = checkAnomalous(newdf.iloc[:,i],newdf[f\"FarAvg{i}\"],newdf[f\"FarSD{i}\"])\n",
        "    newdf[f\"cummulativeseries{i}\"] = recentCumulative(newdf[f\"checkseries{i}\"])\n",
        "\n",
        "# renaming cumulative features\n",
        "newdf.rename(columns={'cummulativeseries0':'cum_PTratio','cummulativeseries1':'cum_reserviourrate','cummulativeseries2':'cum_compressor_eff','cummulativeseries3':'cum_flicker'},inplace=True)\n",
        "\n",
        "#dropping statistic features that are not needed\n",
        "newdf.drop(columns = ['FarAvg0','FarAvg1','FarAvg2','FarAvg3','FarSD0','FarSD1','FarSD2','FarSD3','checkseries0','checkseries1','checkseries2','checkseries3'],inplace=True)\n",
        "newdf.drop(range(30000), inplace=True)\n",
        "# scaling each feature to be normalized\n",
        "scaler = MinMaxScaler()\n",
        "features = [\"power_temp_ratio\",\"reserviour_flowrate\",\"compressor_eff\",\"flicker\",'cum_PTratio','cum_reserviourrate','cum_compressor_eff','cum_flicker']\n",
        "newdf[features] = scaler.fit_transform(newdf[features])\n",
        "\n",
        "###NEED TO CONVERT DAYS TO WEEKS AND CAP IT at 4 weeks\n",
        "dayscolindex = newdf.columns.get_loc(\"Days2Failure\")\n",
        "for i in range(len(newdf)):\n",
        "    if newdf.iloc[i,dayscolindex] > 28:\n",
        "        newdf.iat[i,dayscolindex] = 4\n",
        "    elif newdf.iloc[i,dayscolindex] >21:\n",
        "        newdf.iat[i,dayscolindex] = 3\n",
        "    elif newdf.iloc[i,dayscolindex] >14:\n",
        "        newdf.iat[i,dayscolindex] = 2\n",
        "    elif newdf.iloc[i,dayscolindex] >7:\n",
        "        newdf.iat[i,dayscolindex] = 1\n",
        "    elif newdf.iloc[i,dayscolindex] >0:\n",
        "        newdf.iat[i,dayscolindex] = 0\n",
        "    elif newdf.iloc[i,dayscolindex] == 0:\n",
        "        newdf.iat[i,dayscolindex] = 0\n",
        "\n",
        "newdf.rename(columns={\"Days2Failure\":\"Weeks2Failure\"},inplace=True)\n",
        "\n",
        "newdf.to_csv(\"FINALMETRODATA.csv\")"
      ],
      "metadata": {
        "id": "reyNZ30D6A2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NASA\n",
        "*Xinyu & Amelia *\n",
        "\n",
        "Advanced preprocessing involved..."
      ],
      "metadata": {
        "id": "6RzE4rLxpyAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* divide the dataset in DF ways instead of in training and test set\n",
        "* data process: use average value to fill the NULL values; and use epsilon in case that the mean sqaure root be zero to be divided;\n",
        "* save the data to data/processed root\n",
        "\n",
        "（note）The IO files are based on local machine, regardless of onedrive\n",
        "\n",
        "*Note: might need to fully translate all comments for final hand-in*"
      ],
      "metadata": {
        "id": "CNdR9ymapfgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ========== sklearn used only during the scaling phase, but not directly here -> import StandardScaler ==========\n",
        "# Using custom scaling logic: Fill + Normalise + ε smoothing\n",
        "\n",
        "# Corrected column names from s1 to s21 (21 sensors in total)\n",
        "COLS = ['unit', 'cycle', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 22)]\n",
        "SENSOR_COLS = [f's{i}' for i in range(1, 22)]\n",
        "ALL_FEATS = ['op1', 'op2', 'op3'] + SENSOR_COLS\n",
        "\n",
        "\n",
        "def get_data_dir():\n",
        "    \"\"\"\n",
        "    Locate the NASA CMAPSS data directory within Google Colab  # delete for final submission\n",
        "    \"\"\"\n",
        "    data_dir = '/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA' # delete for final submission\n",
        "    return data_dir # delete for final submission\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    自动定位 data/NASA 目录（相对于项目根目录）(D)\n",
        "    Automatically locate the data/NASA directory (relative to the project root directory)\n",
        "    [REPLACE THIS TEXT WITH END-DOCSTRING]\n",
        "    this_dir = os.path.dirname(__file__)\n",
        "    project_root = os.path.abspath(os.path.join(this_dir, os.pardir, os.pardir))\n",
        "    data_dir = os.path.join(project_root, 'data', 'NASA')\n",
        "    return data_dir\n",
        "    \"\"\"\n",
        "\n",
        "def read_fd(fd: str):\n",
        "    \"\"\"\n",
        "    读取 train 和 test 数据 (D)\n",
        "    Read train_fd and test_fd files\n",
        "    \"\"\"\n",
        "    data_dir = get_data_dir()\n",
        "    train_path = os.path.join(data_dir, f\"train_{fd}.txt\")\n",
        "    test_path  = os.path.join(data_dir, f\"test_{fd}.txt\")\n",
        "    train = pd.read_csv(train_path, sep=r'\\s+', header=None, names=COLS)\n",
        "    test  = pd.read_csv(test_path,  sep=r'\\s+', header=None, names=COLS)\n",
        "    # 有些列可能全是空值，dropna 会删除这些列。这里可以保守处理。(D)\n",
        "    # Some columns may contain only null values; dropna will remove these columns. A conservative approach may be adopted here. --> is this Chat-GPT? (D)\n",
        "    train = train.dropna(axis=1, how='all')\n",
        "    test  = test.dropna(axis=1, how='all')\n",
        "    return train, test\n",
        "\n",
        "def read_rul(fd: str):\n",
        "    \"\"\"\n",
        "    读取 RUL 文件 /\n",
        "    Read official RUL file\n",
        "    \"\"\"\n",
        "    data_dir = get_data_dir()\n",
        "    rul_path = os.path.join(data_dir, f\"RUL_{fd}.txt\")\n",
        "    rul = pd.read_csv(rul_path, header=None, names=['RUL'])\n",
        "    return rul\n",
        "\n",
        "def add_train_rul(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    给训练集每行添加 RUL 列 / Add RUL to training set\n",
        "\n",
        "    Add an RUL column to each row in the training set\n",
        "    \"\"\"\n",
        "    max_cyc = df.groupby('unit')['cycle'].max().rename('max_cycle')\n",
        "    df = df.merge(max_cyc, on='unit', how='left')\n",
        "    df['RUL'] = df['max_cycle'] - df['cycle']\n",
        "    df = df.drop(columns=['max_cycle'])\n",
        "    return df\n",
        "\n",
        "def build_test_labels(test_df: pd.DataFrame, rul_vec: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    用官方 RUL 和测试集最后时刻构造测试标签 / Build test labels per unit\n",
        "    返回 DataFrame 包含 ['unit','RUL'] (D)\n",
        "\n",
        "    Construct test labels per unit using official RUL and test set at the last moment\n",
        "    Return a DataFrame containing [“unit”,'RUL']\n",
        "    \"\"\"\n",
        "    last = test_df.groupby('unit')['cycle'].max().rename('last_cycle').reset_index()\n",
        "    last = last.sort_values('unit').reset_index(drop=True)\n",
        "    last['RUL'] = rul_vec['RUL'].values\n",
        "    return last[['unit', 'RUL']]\n",
        "\n",
        "def scale_by_train(train_df: pd.DataFrame, test_df: pd.DataFrame, eps: float = 1e-8):\n",
        "    \"\"\"\n",
        "    填补缺失 + 标准化 + ε 平滑\n",
        "    返回 train_scaled, test_scaled, scaler_params = (means, stds_adj) (D)\n",
        "\n",
        "    Fill missing values, scale & standardise features, complete epsilon smoothing (to avoid dividing by zero)\n",
        "    Return train_scaled, test_scaled, scaler_params = (means, stds_adj)\n",
        "    \"\"\"\n",
        "    train_scaled = train_df.copy()\n",
        "    test_scaled = test_df.copy()\n",
        "\n",
        "    # —— 1. 均值填补（用训练集均值填补 train & test 的缺失值）(D)\n",
        "    # —— 1. Mean imputation (filling missing values in both training and test datasets with the mean from the training set)\n",
        "    feat_means = train_df[ALL_FEATS].mean()\n",
        "    train_scaled[ALL_FEATS] = train_scaled[ALL_FEATS].fillna(feat_means)\n",
        "    test_scaled[ALL_FEATS]  = test_scaled[ALL_FEATS].fillna(feat_means)\n",
        "\n",
        "    # —— 2. 计算标准差 & 平滑 (D)\n",
        "    # —— 2. Calculating standard deviation & smoothing\n",
        "    stds = train_scaled[ALL_FEATS].std(ddof=0)\n",
        "    stds_adj = stds.copy()\n",
        "    stds_adj[stds_adj < eps] = eps\n",
        "\n",
        "    # —— 3. 标准化 (x - mean) / stds_adj (D)\n",
        "    # —— 3. Normalise (x - mean) / stds_adj\n",
        "    for feat in ALL_FEATS:\n",
        "        train_scaled[feat] = (train_scaled[feat] - feat_means[feat]) / stds_adj[feat]\n",
        "        test_scaled[feat]  = (test_scaled[feat]  - feat_means[feat]) / stds_adj[feat]\n",
        "\n",
        "    scaler_params = (feat_means, stds_adj)\n",
        "    return train_scaled, test_scaled, scaler_params\n",
        "\n",
        "def process_fd(fd: str):\n",
        "    \"\"\"\n",
        "    为一个 FD 子集做完整处理（读取、加 RUL、缩放、保存 (D)\n",
        "    Process one FD subset end-to-end; perform complete processing on an FD subset (read, add RUL, scale, save)\n",
        "    \"\"\"\n",
        "    train, test = read_fd(fd)\n",
        "    rul_test = read_rul(fd)\n",
        "\n",
        "    train = add_train_rul(train)\n",
        "    test_labels = build_test_labels(test, rul_test)\n",
        "\n",
        "    # 特征工程可在这里插入 -> Feature engineering can be inserted here. (D)\n",
        "\n",
        "    train_scaled, test_scaled, scaler_params = scale_by_train(train, test)\n",
        "\n",
        "    # 提取 X, y (D)\n",
        "    # Extract X, y\n",
        "    X_train = train_scaled[['unit', 'cycle'] + ALL_FEATS].copy()\n",
        "    y_train = train_scaled['RUL'].copy()\n",
        "    X_test  = test_scaled[['unit', 'cycle'] + ALL_FEATS].copy()\n",
        "    y_test  = test_labels.set_index('unit')['RUL']\n",
        "\n",
        "    # 保存路径 (D)\n",
        "    # Save path\n",
        "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))\n",
        "    output_dir = os.path.join(project_root, 'data', 'processed', fd)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # 保存 CSV (D)\n",
        "    # Save .csv\n",
        "    X_train.to_csv(os.path.join(output_dir, f\"X_train_{fd}.csv\"), index=False)\n",
        "    y_train.to_csv(os.path.join(output_dir, f\"y_train_{fd}.csv\"), index=False)\n",
        "    X_test.to_csv(os.path.join(output_dir, f\"X_test_{fd}.csv\"), index=False)\n",
        "    test_labels.to_csv(os.path.join(output_dir, f\"y_test_{fd}_units.csv\"), index=False)\n",
        "\n",
        "    print(f\"[{fd}] processed. Saved to {output_dir}. Scaler params (means, stds_adj):\")\n",
        "    # 可以只打印部分 params，以避免输出过多内容 (D)\n",
        "    # You may choose to print only a subset of the parameters to avoid outputting excessive content. --> is this Chat GPT? (D)\n",
        "    means, stds_adj = scaler_params\n",
        "    print(\" means (first 5):\", means.head(5).to_dict())\n",
        "    print(\" stds_adj (first 5):\", stds_adj.head(5).to_dict())\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    主流程入口 / Main entry: 可以处理多个 FD 子集 (D)\n",
        "    Main entry: Capable of handling multiple subsets of FD\n",
        "    \"\"\"\n",
        "    fds = [\"FD001\", \"FD002\", \"FD003\", \"FD004\"]\n",
        "    for fd in fds:\n",
        "        print(\"Processing\", fd)\n",
        "        try:\n",
        "            process_fd(fd)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {fd}:\", e)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "LOtgyuR313Rj",
        "outputId": "eaed098d-a860-487e-c7f6-64b271ecc557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing FD001\n",
            "Error processing FD001: name '__file__' is not defined\n",
            "Processing FD002\n",
            "Error processing FD002: name '__file__' is not defined\n",
            "Processing FD003\n",
            "Error processing FD003: name '__file__' is not defined\n",
            "Processing FD004\n",
            "Error processing FD004: name '__file__' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amelia's notes for pre-processing the NASA dataset\n",
        "**RQ2: Can the failure of a mechanical system be predicted based on the manufacturing quality of the internal components within it?**\n",
        "* inspect whether there is a link between manufacturing quality and frequency of failure within a mechanical system\n",
        "\n",
        "Manufacturing quality can be inferred from how each engine behaves at the start of life and how quickly it degrades relative to others. Thus, our advanced data pre-processing should be focused on extracting features that represent each engines inherent condition and comparing them with how long it lasts (RUL).\n",
        "\n",
        "Possible steps for feature extraction:\n",
        "\n",
        "\n",
        "1. Computing the mean sensor readings from each engine's initial cycles.\n",
        "\n",
        "    - Initial cycles should be considerd as roughly the first 5-10 cycles\n",
        "    - Mean sensor readings act as a **baseline** for each engine unit: they capture variations caused by manufactoring or assembly differences\n",
        "2. For each sensor, then measure the deviaton from that engine's baseline.\n",
        "    - Any deviations will help to isolate degradation relative to the initial quality of the engine (therefore meaning that our ML models can differentiate a normal reading from an abnormal reading for any specific engine unit)\n",
        "3. Define some early-life window (~20% through an engine's lifespan) and compute a slope (per engine unit) for key sensors to measure intial degradation rates.  \n",
        "    - These rates will measure how robust each engine's components are.\n",
        "      * E.g., high slopes = fast change, therefore indicating poor manufactoring quality\n",
        "\n",
        "4. Compute some health efficiency ratios (assuming that manufactoring defects will manifest as inefficiency in energy conversion) *See end section for further notes on features selected to calculate these ratios*\n",
        "    - pressure ratio → P30/P2: $(\\dfrac{total\\ pressure\\ at\\ HPC\\ outlet}{pressure\\ at\\ fan\\ inlet})$\n",
        "\n",
        "    - temp ratio → T30/T24: $(\\dfrac{total\\ temperature\\ at\\ HPC\\ outlet}{total\\ temperature\\ at\\ LPC\\ outlet})$\n",
        "\n",
        "    - ∴ efficiency = $(\\dfrac{pressure\\ ratio}{temp\\ ratio})$\n",
        "\n",
        "5. Compare lifetime & degradation features with each engine unit's RUL to inspect whether engines with worse baselines fail earlier\n",
        "    - At unit level this includes initial efficiency, initial temperature ratio & initial motor current\n",
        "      * *models built per time-step/cycle should include these as static features repeated for that engine unit: assumption that they represent inherent quality*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "#### Ratio calculation notes (for step 4)\n",
        "*Please review this!! I'm not certain of how true this is!!*\n",
        "\n",
        "  **air-flow stages in a turbofan jet engine:**\n",
        "  1. Fan (low pressure section): brings in large volumes of air\n",
        "  2. Low-pressure compressor (LPC): compresses air moderately\n",
        "  3. High-pressure compressor (HPC): compresses air further before combustion\n",
        "  4. Combustor: mixes compressed air w/fuel and ignites it\n",
        "  5. Turbines: extracts power to drive the compressors and fan\n",
        "\n",
        "**P30/P2**, where:\n",
        "\n",
        "→ P2 = pressure at fan inlet ⇒ ambient intake conditions\n",
        "\n",
        "→ P30 = pressure at HPC outlet ⇒ pressure just before the combustor\n",
        "\n",
        "P30/P2 ratio acts as a proxy for overall compressor pressure, and hence is a direct measure of compressor performance and efficiency:\n",
        "* as compressor degrades its ability to raise the pressure drops\n",
        "* subsequently P30/P2 gradually declines\n",
        "\n",
        "**T30/T24**, where:\n",
        "\n",
        "→ T24 = temp at LPC outlet ⇒ air after the first stage of compression\n",
        "\n",
        "→ T30 = temp at HPC outlet ⇒ air just before the combustor\n",
        "\n",
        "T30/T24 ratio approximates the temperature rise across the HPC, therefore mirroring how much work the HPC is doing\n",
        "* as the compressor loses efficiency its temperature rise across those stages changes\n",
        "    - rise: more work is needed for the same compression ratio\n",
        "\n",
        "\n",
        "**(P30/P2)/(T30/T24)**\n",
        "\n",
        "The pressure & temperature ratios together capture the compressor's \"thermodynamic signature\"\n",
        "\n",
        "→ P30/P2 = pressure efficiency\n",
        "\n",
        "→ T30/T24 = temperature efficiency/work requirement\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9SRNp-pePGfy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2hPEH_56RWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}