{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosfet-sudo/ML-Project-G75/blob/main/90049_A2_G75_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "note: the github must be public in order for us to view 'live updates' of any changes made to this notebook"
      ],
      "metadata": {
        "id": "m5JmuJbLWmzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive\n",
        "Run the code below to mount your google drive to this notebook\n",
        "Before doing this, please be sure that you have accepted Amelia's invitation to access/author the google drive file 'IML_G75_Colab_Notebooks'"
      ],
      "metadata": {
        "id": "8bYR581d0sY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ru_ouPJqYHq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MScFRuDoWV4a",
        "outputId": "5c9a87d6-e7ec-4f88-e4b5-5301cf2e875f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Data Processing\n",
        "(Done by Elijah)\n",
        "Each Dataset was checked for duplicate entries, and then specific numeric values were normalized. In addition to this there was some minor text processing of the NASA dataset where the text files were tranferred into csv format. Furthermore adjustments were made to the metrodataset where all data points of each minute were averaged in order to make the dataset smaller. In particular for values that were binary (i.e) 1 or 0 if the average was strictly greater than 0.5 then it was set to 1 otherwise it was set to 0. This averaging should reduce noise and reduce the affect of anomalous outliers."
      ],
      "metadata": {
        "id": "-glSLfYeXa2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metro-PT3"
      ],
      "metadata": {
        "id": "fILNNgYJYIM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "## averaging function for new dataset that preserves when errors occur, n must be even n=6 for averaging every minute\n",
        "## takes the data  frame the number of samples to bunch together and average and the indexes that are we are applying the averaging off.\n",
        "## the inputs are the data frame, an integer n, and 2 arrays of numeric indexes it returns the new (smaller) averaged dataframe\n",
        "def timeAvg(df, n, tempindexes, elec_temp_indexes):\n",
        "    m = n/2\n",
        "    m = int(m)\n",
        "\n",
        "    df_empty = df.copy(deep=True).iloc[0:0]\n",
        "    avg_atpoints = []\n",
        "    max_atpoints = []\n",
        "    for i in tempindexes:\n",
        "        print(i)\n",
        "        avg_atpoints = []\n",
        "        temp_series = df.iloc[:,i]\n",
        "        numrows = int(temp_series.shape[0])\n",
        "        for j in range(m, int(numrows-m),n):\n",
        "            avg_atpoints.append(temp_series.iloc[j-m:j+m].mean()) #here we take the average\n",
        "        df_empty.iloc[:,i] = avg_atpoints\n",
        "    for i in elec_temp_indexes:\n",
        "        print(i)\n",
        "        max_atpoints = []\n",
        "        temp_series = df.iloc[:,i]\n",
        "        numrows = int(temp_series.shape[0])\n",
        "        for j in range(m, int(numrows-m),n):\n",
        "            avg = temp_series.iloc[j-m:j+m].mean()  #we use the average to decide what to assign to this datapoint\n",
        "            if avg > 0.5:\n",
        "                max_atpoints.append(1)\n",
        "            else:\n",
        "                max_atpoints.append(0)\n",
        "        df_empty.iloc[:,i] = max_atpoints\n",
        "    df_empty.iloc[:,0] = list(range(1,df_empty.shape[0]+1))\n",
        "    return df_empty\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/MetroPT3(AirCompressor).csv\")\n",
        "#Duplicate checking\n",
        "\n",
        "df = df.drop(columns = [\"timestamp\"])\n",
        "num_columns = df.shape[1]\n",
        "num_rows = df.shape[0]\n",
        "duplicateddf=df.duplicated()\n",
        "no_duplicate_df = df.loc[duplicateddf==False]\n",
        "\n",
        "\n",
        "no_duplicate_df.rename(columns={\"Unnamed: 0\": \"Time Elapased [seconds]\"}, inplace=True)\n",
        "print(no_duplicate_df.columns)\n",
        "\n",
        "#Normalization\n",
        "# Find the indexes of columns which have values we want to normalize\n",
        "temp_indexes = [no_duplicate_df.columns.get_loc(\"Time Elapased [seconds]\")] ##SHOULD WE NORMALIZE THIS?\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"TP2\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"TP3\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"H1\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"DV_pressure\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Reservoirs\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Oil_temperature\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Motor_current\"))\n",
        "\n",
        "\n",
        "# BOTH RPM AND TOOLWEAR HAVE ISSUES WITH CERTAIN DTYPES BEING INT DUE TO WHOLE NUMBERS IN DATA\n",
        "no_duplicate_df[\"Time Elapased [seconds]\"] = no_duplicate_df[\"Time Elapased [seconds]\"].astype(float)\n",
        "\n",
        "#for each index normalize the values and for each row set the value to the normalized value\n",
        "for i in temp_indexes:\n",
        "    temp_series = no_duplicate_df.iloc[:,i]\n",
        "    series_max = temp_series.max()\n",
        "    series_min = temp_series.min()\n",
        "    series_range = temp_series.max() - temp_series.min()\n",
        "\n",
        "    for j in range(temp_series.shape[0]):\n",
        "\n",
        "        no_duplicate_df.iat[j,i] = (no_duplicate_df.iat[j,i]-series_min)/series_range\n",
        "\n",
        "\n",
        "\n",
        "## new temp indexes for finding columns of electrical signal data\n",
        "elec_temp_indexes = [no_duplicate_df.columns.get_loc(\"COMP\")]\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"DV_eletric\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Towers\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"MPG\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"LPS\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Pressure_switch\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Oil_level\"))\n",
        "elec_temp_indexes.append(no_duplicate_df.columns.get_loc(\"Caudal_impulses\"))\n",
        "\n",
        "\n",
        "\n",
        "no_dupe_normalized_df = no_duplicate_df\n",
        "no_dupe_normalized_df_avg = timeAvg(no_dupe_normalized_df,6,temp_indexes,elec_temp_indexes)\n",
        "\n",
        "no_dupe_normalized_df_avg.to_csv(\"METROPT3_AVGMINUTEPROCESSED.csv\")"
      ],
      "metadata": {
        "id": "-JPJp2XBYLL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NASA"
      ],
      "metadata": {
        "id": "iXTnSmEoYMNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Method for parsing text files into dataframe, takes a string input and returns a dataframe object\n",
        "def parseTxt2Dataframe(String):\n",
        "    with open(String, 'r', encoding='utf-8') as file:\n",
        "        lines = []\n",
        "        for line in file:\n",
        "            newline = line.strip()\n",
        "            newline = newline.split()\n",
        "            lines.append(newline)\n",
        "        new_df = pd.DataFrame(lines, columns=['Unit Number', 'Time in cycles',\n",
        "                                  'Operational Setting 1', 'Operational Setting 2','Operational Setting 3',\n",
        "                                  'T2','T24','T30','T50','P2','P15','P30','Nf','Nc','epr','Ps30','phi','NRf','NRc',\n",
        "                                  'BPR','farB','htBleed','Nf_dmd','PCNfr_dmd','W31','W32'])\n",
        "        num_cols = new_df.shape[1]\n",
        "        for i in range(num_cols):\n",
        "            new_df.iloc[:,i] = pd.to_numeric(new_df.iloc[:,i])\n",
        "\n",
        "\n",
        "    return new_df\n",
        "\n",
        "#normalizes the data, and checks for duplicates, takes dataframe object as input and returns a dataframe\n",
        "def normalize_and_checkdupe(df):\n",
        "    checkdf = df.drop(columns=['Unit Number','Time in cycles'])\n",
        "    duplicateddf = checkdf.duplicated()\n",
        "    no_dupe_df = df.loc[duplicateddf==False]\n",
        "    num_cols = no_dupe_df.shape[1]\n",
        "    num_rows = no_dupe_df.shape[0]\n",
        "    for i in range(2,num_cols):\n",
        "        max_val = no_dupe_df.iloc[:,i].max()\n",
        "        min_val = no_dupe_df.iloc[:,i].min()\n",
        "        val_range = max_val - min_val\n",
        "        for j in range(num_rows):\n",
        "            if val_range != 0:\n",
        "                no_dupe_df.iat[j, i] = (no_dupe_df.iat[j, i] - min_val) / val_range\n",
        "            else:\n",
        "                no_dupe_df.iat[j, i] = -1\n",
        "\n",
        "    return no_dupe_df\n",
        "\n",
        "\n",
        "# parsing each text file into a dataframe\n",
        "df_test1 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD001.txt')\n",
        "df_test2 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD002.txt')\n",
        "df_test3 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD003.txt')\n",
        "df_test4 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/test_FD004.txt')\n",
        "df_train1 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD001.txt')\n",
        "df_train2 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD002.txt')\n",
        "df_train3 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD003.txt')\n",
        "df_train4 = parseTxt2Dataframe('/content/drive/My Drive/IML_G75_Colab_Notebooks/CMaps_NASA/train_FD004.txt')\n",
        "\n",
        "\n",
        "\n",
        "#combining the data into one dataset\n",
        "\n",
        "df_all = pd.concat([df_test1,df_test2,df_test3,df_test4,df_train1,df_train2,df_train3,df_train4],ignore_index=True)\n",
        "print(df_all.shape)\n",
        "print(df_test1.shape)\n",
        "\n",
        "## normalizing and checking for duplicates when combined\n",
        "df_all_processed = normalize_and_checkdupe(df_all)\n",
        "\n",
        "## normalizing and checking for duplicates when uncombined\n",
        "df_test1_processed = normalize_and_checkdupe(df_test1)\n",
        "#df_test2_processed = normalize_and_checkdupe(df_test2)\n",
        "#df_test3_processed = normalize_and_checkdupe(df_test3)\n",
        "#df_test4_processed = normalize_and_checkdupe(df_test4)\n",
        "#df_train1_processed = normalize_and_checkdupe(df_train1)\n",
        "#df_train2_processed = normalize_and_checkdupe(df_train2)\n",
        "#df_train3_processed = normalize_and_checkdupe(df_train3)\n",
        "#df_train4_processed = normalize_and_checkdupe(df_train4)\n",
        "\n",
        "\n",
        "\n",
        "## testing and debug\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(df_all_processed.shape)\n",
        "print(df_all_processed.head())\n",
        "print(df_test1_processed.head())\n",
        "print(\"newline\")\n",
        "\n",
        "\n",
        "print(\"newline\")\n",
        "print(df_all_processed.shape)\n",
        "\n",
        "pd.reset_option('display.max_columns')\n",
        "df_all_processed.to_csv('NASAprocessed.csv')"
      ],
      "metadata": {
        "id": "-XhCb94XYPf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PdM"
      ],
      "metadata": {
        "id": "NHgRsQ1jYP9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "df = pd.read_csv(\"C:/Users/ej/PycharmProjects/Assignment2/.venv/Scripts/ai_2020.csv\")\n",
        "#Duplicate checking\n",
        "\n",
        "df = df.drop(columns = [\"UDI\"])\n",
        "num_columns = df.shape[1]\n",
        "num_rows = df.shape[0]\n",
        "duplicateddf=df.duplicated()\n",
        "no_duplicate_df = df.loc[duplicateddf==False]\n",
        "\n",
        "print(no_duplicate_df.columns)\n",
        "#Normalization\n",
        "\n",
        "temp_indexes = [no_duplicate_df.columns.get_loc(\"Air temperature [K]\")]\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Process temperature [K]\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Rotational speed [rpm]\")) #TURN INTO SECONDS?\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Torque [Nm]\"))\n",
        "temp_indexes.append(no_duplicate_df.columns.get_loc(\"Tool wear [min]\"))     # TURN INTO SECONDS?\n",
        "# BOTH RPM AND TOOLWEAR HAVE ISSUES WITH CERTAIN DTYPES BEING INT DUE TO WHOLE NUMBERS IN DATA\n",
        "no_duplicate_df[\"Rotational speed [rpm]\"] = no_duplicate_df[\"Rotational speed [rpm]\"].astype(float)\n",
        "no_duplicate_df[\"Tool wear [min]\"] = no_duplicate_df[\"Tool wear [min]\"].astype(float)\n",
        "\n",
        "for i in temp_indexes:\n",
        "\n",
        "    temp_series = no_duplicate_df.iloc[:,i]\n",
        "    series_max = temp_series.max()\n",
        "    series_min = temp_series.min()\n",
        "    series_range = temp_series.max() - temp_series.min()\n",
        "\n",
        "    for j in range(temp_series.shape[0]):\n",
        "\n",
        "        no_duplicate_df.iat[j,i] = (no_duplicate_df.iat[j,i]-series_min)/series_range\n",
        "\n",
        "\n",
        "no_dupe_normalized_df = no_duplicate_df\n",
        "\n",
        "no_dupe_normalized_df.to_csv(\"PredicitiveMaintenanceDatasetPROCESSED.csv\")"
      ],
      "metadata": {
        "id": "keDopJK_YSRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced pre-processing"
      ],
      "metadata": {
        "id": "stm011aPzdHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metro PT-3\n",
        "Xianyu & Amelia\n",
        "\n",
        "\n",
        "\n",
        "*   How well do electrical load, temperature and output pressure predict compressor stress/ early failure?\n",
        "*   ooga booga\n",
        "\n"
      ],
      "metadata": {
        "id": "I0eJeAs8z4-G"
      }
    }
  ]
}